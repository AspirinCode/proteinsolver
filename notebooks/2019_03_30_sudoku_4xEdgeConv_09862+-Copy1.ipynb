{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_gnhLD0HvhU",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fXJz1l9HvhW"
   },
   "source": [
    "### Install `pytorch_geometric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "hIbvKDk_HvhX",
    "outputId": "349cb6d3-ffbc-4550-d9dd-dd6288ed5356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-scatter\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/83/67eeea00c2db1959e2ff95d8680dbd756977bfab254bda8658f09dc3bc11/torch_scatter-1.1.2.tar.gz\n",
      "Building wheels for collected packages: torch-scatter\n",
      "  Building wheel for torch-scatter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a8/41/ef/1a52be728eedba23aa6d39b0bd6e9b533cb7ff572e967a6a43\n",
      "Successfully built torch-scatter\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "tw-y9MmIHvha",
    "outputId": "0666864a-5017-40a1-a77c-d23388eb0b1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-sparse\n",
      "  Downloading https://files.pythonhosted.org/packages/73/72/e374662f6f47d9ac0e082a6d5c18d14e15c52863e89c6bc6957a0d2ed026/torch_sparse-0.2.4.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse) (1.14.6)\n",
      "Building wheels for collected packages: torch-sparse\n",
      "  Building wheel for torch-sparse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/14/8f/77/7be0a2e42c5d1a6257b71edf5dc42f9e148d2137b66d869ee0\n",
      "Successfully built torch-sparse\n",
      "Installing collected packages: torch-sparse\n",
      "Successfully installed torch-sparse-0.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "n3SMtaYHHvhc",
    "outputId": "6250a80b-b32d-4fc8-cf90-ad0507e83ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-cluster\n",
      "  Downloading https://files.pythonhosted.org/packages/32/c8/9b3af10be647326dd807bb2fe7ced8ae4c3fd74178dba884621749afc4d7/torch_cluster-1.2.4.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from torch-cluster) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-cluster) (1.14.6)\n",
      "Building wheels for collected packages: torch-cluster\n",
      "  Building wheel for torch-cluster (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/5b/54/4a/ae0125851936c2b5ea13dddb1ab4b103b9f71e0f3211a6c4e1\n",
      "Successfully built torch-cluster\n",
      "Installing collected packages: torch-cluster\n",
      "Successfully installed torch-cluster-1.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "4AQ_A4XLHvhg",
    "outputId": "601b9d38-8291-431d-a90b-046bb233469a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-spline-conv\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/6d/b34721af4bb907814a41a1e0e5e426c97aee644efef6877ed112bfb3d81c/torch_spline_conv-1.0.6.tar.gz\n",
      "Building wheels for collected packages: torch-spline-conv\n",
      "  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f6/e2/d7/d87645a1e1d34b51633b7fd13f020b618c0ee84000b4f4085c\n",
      "Successfully built torch-spline-conv\n",
      "Installing collected packages: torch-spline-conv\n",
      "Successfully installed torch-spline-conv-1.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-spline-conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "colab_type": "code",
    "id": "uk6E9JP_Hvhj",
    "outputId": "5a986a43-4d09-41ab-efb6-9fb1cab56cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/4d/06dd0d277bf82ff4e1888f73b4c522c35f017505acacc25ecc95befaac6e/torch_geometric-1.1.0.tar.gz (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 2.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.14.6)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.1.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.20.3)\n",
      "Collecting plyfile (from torch-geometric)\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/82/d4069cbb49954d44087c37ff616cb423d3e2c0dd276378cdb4af3e3ef2ee/plyfile-0.7.tar.gz\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.0)\n",
      "Collecting rdflib (from torch-geometric)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
      "\u001b[K    100% |████████████████████████████████| 348kB 24.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.5.3)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.3.1)\n",
      "Collecting isodate (from rdflib->torch-geometric)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 14.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas->torch-geometric) (1.11.0)\n",
      "Building wheels for collected packages: torch-geometric, plyfile\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3e/c1/87/daa10b8fa568fb9a84ff1d9c24582af1ecc7e797e5e69ff58e\n",
      "  Building wheel for plyfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/91/3e/ee/e5630ef0fd53cedaa6e911ba27e8b40fff034388d1f264bb92\n",
      "Successfully built torch-geometric plyfile\n",
      "Installing collected packages: plyfile, isodate, rdflib, torch-geometric\n",
      "Successfully installed isodate-0.6.0 plyfile-0.7 rdflib-4.2.2 torch-geometric-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9q4iTifHvhl"
   },
   "source": [
    "### Install `proteinsolver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "colab_type": "code",
    "id": "HCWw1IgNHvhm",
    "outputId": "fcb1a352-1ca8-4708-c3af-11bb0f9a3c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://gitlab%2Bdeploy-token-52110:****@gitlab.com/ostrokach/proteinsolver.git\n",
      "  Cloning https://gitlab%2Bdeploy-token-52110:****@gitlab.com/ostrokach/proteinsolver.git to /tmp/pip-req-build-u2bdbexg\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: proteinsolver\n",
      "  Building wheel for proteinsolver (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-6rtg4lok/wheels/fb/05/26/209b40c24fc617333b311553f1c37610fb9b3475d67811bfcc\n",
      "Successfully built proteinsolver\n",
      "Installing collected packages: proteinsolver\n",
      "Successfully installed proteinsolver-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://gitlab.com/ostrokach/proteinsolver.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eoBMUoW2Hvhp",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "EmCKag-5MCiB",
    "outputId": "40059204-55bf-4285-eb0a-34098e980c40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  5 11:27:04 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN V             On   | 00000000:05:00.0  On |                  N/A |\n",
      "| 54%   76C    P2   133W / 250W |  10984MiB / 12033MiB |     97%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN Xp            On   | 00000000:06:00.0 Off |                  N/A |\n",
      "| 23%   30C    P8     8W / 250W |     12MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN Xp            On   | 00000000:09:00.0 Off |                  N/A |\n",
      "| 23%   26C    P8     9W / 250W |     12MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      6465      C   ...apkg-adjacency-net-v2-test81/bin/python 10853MiB |\n",
      "|    0      6913      G   /usr/lib/xorg/Xorg                            39MiB |\n",
      "|    0      6957      G   /usr/bin/gnome-shell                          78MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbKxMUZWHvhq"
   },
   "outputs": [],
   "source": [
    "import atexit\n",
    "import csv\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from collections import deque\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import ChebConv, EdgeConv, GATConv, GCNConv\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, scatter_\n",
    "\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "rczy7pPiHvhs",
    "outputId": "8e4673d7-4cc7-4f8e-ffc4-de94302e5fab"
   },
   "outputs": [],
   "source": [
    "import proteinsolver\n",
    "import proteinsolver.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fFnAyUOHvhv"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_NAME = \"protein_4xEdgeConv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/kimlab1/strokach/working/proteinsolver/notebooks/protein_4xEdgeConv')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOTEBOOK_PATH = Path(NOTEBOOK_NAME).resolve()\n",
    "NOTEBOOK_PATH.mkdir(exist_ok=True)\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsEY3dtLHvhy",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/strokach/ml_data')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT = Path(tempfile.gettempdir())\n",
    "DATA_ROOT = Path(\"/home/strokach/ml_data\")\n",
    "DATA_ROOT.mkdir(exist_ok=True)\n",
    "DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_dataset_train_0 = proteinsolver.datasets.ProteinDataset(root=DATA_ROOT / \"protein_train_0\", subset=\"train_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_dataset_train_1 = proteinsolver.datasets.ProteinDataset(root=DATA_ROOT / \"protein_train_1\", subset=\"train_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protein_dataset_train_2 = proteinsolver.datasets.ProteinDataset(root=DATA_ROOT / \"protein_train_2\", subset=\"train_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_dataset_valid = proteinsolver.datasets.ProteinInMemoryDataset(root=DATA_ROOT / \"protein_valid\", subset=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_dataset_test = proteinsolver.datasets.ProteinInMemoryDataset(root=DATA_ROOT / \"protein_test\", subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.1787e-02, 2.7956e-07])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_dataset_train_0[1200].edge_attr.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3435e+02, -1.2324e-04])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_dataset_valid[5].edge_attr.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2734, 0.2666])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_dataset_valid[5].edge_attr.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3892,  0.0147],\n",
       "        [-0.1799,  0.0294],\n",
       "        [-0.2038,  0.0440],\n",
       "        ...,\n",
       "        [-0.3892, -0.0147],\n",
       "        [-0.2563, -0.0294],\n",
       "        [-0.3892, -0.0147]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_dataset_valid[5].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([228., 508., 258., 238., 302., 602., 470., 536., 576., 692.]),\n",
       " array([-0.39062133, -0.30164394, -0.21266654, -0.12368914, -0.03471174,\n",
       "         0.05426566,  0.14324306,  0.23222046,  0.32119787,  0.41017526,\n",
       "         0.49915266], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(protein_dataset_valid[1].edge_attr[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_attr_lst = []\n",
    "# for i in range(0, 100_000):\n",
    "#     data = protein_dataset_train_0[i]\n",
    "#     edge_attr_lst.append(data.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_attr = torch.cat(edge_attr_lst, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(edge_attr[:, 0], bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SudokuDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZP3bLDO9PgT9"
   },
   "outputs": [],
   "source": [
    "sudoku_dataset_train = proteinsolver.datasets.SudokuDataset2(root=DATA_ROOT.joinpath(\"sudoku_train\"), subset=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudoku_dataset_valid = proteinsolver.datasets.SudokuDataset2(root=DATA_ROOT.joinpath(\"sudoku_valid\"), subset=\"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6W1sfkUHr89O"
   },
   "source": [
    "## `TUDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lOt0mVyir89P"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "Ctf7-3yYr89S",
    "outputId": "9ab47d4d-5cad-4f36-b32c-0f7b912d0e0a"
   },
   "outputs": [],
   "source": [
    "tu_dataset = TUDataset(root=tempfile.gettempdir() + '/ENZYMES', name='ENZYMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "tLtEuvRwr89Y",
    "outputId": "ca22011f-8035-4c70-c722-4ed39b1bdc17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 168], x=[37, 3], y=[1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3kWuRaxr89h"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72WWaQOXJDdr"
   },
   "outputs": [],
   "source": [
    "class EdgeConvMod(torch.nn.Module):\n",
    "    def __init__(self, nn, aggr=\"max\"):\n",
    "        super().__init__()\n",
    "        self.nn = nn\n",
    "        self.aggr = aggr\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        \"\"\"\"\"\"\n",
    "        row, col = edge_index\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "\n",
    "        # TODO: Try -x[col] instead of x[col] - x[row]\n",
    "        if edge_attr is None:\n",
    "            out = torch.cat([x[row], x[col]], dim=-1)\n",
    "        else:\n",
    "            out = torch.cat([x[row], x[col], edge_attr], dim=-1)\n",
    "        out = self.nn(out)\n",
    "        x = scatter_(self.aggr, out, row, dim_size=x.size(0))\n",
    "\n",
    "        return x, out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}(nn={})\".format(self.__class__.__name__, self.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDrlxheizsKg"
   },
   "outputs": [],
   "source": [
    "class EdgeConvBatch(nn.Module):\n",
    "    def __init__(self, gnn, hidden_size, batch_norm=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gnn = gnn\n",
    "\n",
    "        x_post_modules = []\n",
    "        edge_attr_post_modules = []\n",
    "\n",
    "        if batch_norm is not None:\n",
    "            x_post_modules.append(nn.BatchNorm1d(hidden_size))\n",
    "            edge_attr_post_modules.append(nn.BatchNorm1d(hidden_size))\n",
    "\n",
    "        if dropout:\n",
    "            x_post_modules.append(nn.Dropout(dropout))\n",
    "            edge_attr_post_modules.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.x_postprocess = nn.Sequential(*x_post_modules)\n",
    "        self.edge_attr_postprocess = nn.Sequential(*edge_attr_post_modules)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        x, edge_attr = self.gnn(x, edge_index, edge_attr)\n",
    "        x = self.x_postprocess(x)\n",
    "        edge_attr = self.edge_attr_postprocess(edge_attr)\n",
    "        return x, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNCaSlrFDGA6"
   },
   "outputs": [],
   "source": [
    "def get_graph_conv_layer(input_size, hidden_size, output_size):\n",
    "    mlp = nn.Sequential(\n",
    "        #\n",
    "        nn.Linear(input_size, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, output_size),\n",
    "    )\n",
    "    gnn = EdgeConvMod(nn=mlp, aggr=\"add\")\n",
    "    graph_conv = EdgeConvBatch(gnn, output_size, batch_norm=True, dropout=0.2)\n",
    "    return graph_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNCaSlrFDGA6"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, x_input_size, adj_input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_x = nn.Sequential(\n",
    "            nn.Embedding(x_input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "#             nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        if adj_input_size:\n",
    "            self.embed_adj = nn.Sequential(\n",
    "                nn.Linear(adj_input_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "#                 nn.ReLU(),\n",
    "            )\n",
    "        else:\n",
    "            self.embed_adj = None\n",
    "\n",
    "        self.graph_conv_1 = get_graph_conv_layer((2 + bool(adj_input_size)) * hidden_size, 2 * hidden_size, hidden_size)\n",
    "        self.graph_conv_2 = get_graph_conv_layer(3 * hidden_size, 2 * hidden_size, hidden_size)\n",
    "        self.graph_conv_3 = get_graph_conv_layer(3 * hidden_size, 2 * hidden_size, hidden_size)\n",
    "        self.graph_conv_4 = get_graph_conv_layer(3 * hidden_size, 2 * hidden_size, hidden_size)\n",
    "        self.linear_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "\n",
    "        x = self.embed_x(x)\n",
    "        edge_index, _ = remove_self_loops(edge_index)  # We should remove self loops in this case!\n",
    "        edge_attr = self.embed_adj(edge_attr) if edge_attr is not None else None\n",
    "\n",
    "        x_out, edge_attr_out = self.graph_conv_1(x, edge_index, edge_attr)\n",
    "        x += x_out\n",
    "        edge_attr = (edge_attr + edge_attr_out) if edge_attr is not None else edge_attr_out\n",
    "\n",
    "        x = F.relu(x)\n",
    "        edge_attr = F.relu(edge_attr)\n",
    "        x_out, edge_attr_out = self.graph_conv_2(x, edge_index, edge_attr)\n",
    "        x += x_out\n",
    "        edge_attr += edge_attr_out\n",
    "\n",
    "        x = F.relu(x)\n",
    "        edge_attr = F.relu(edge_attr)\n",
    "        x_out, edge_attr_out = self.graph_conv_3(x, edge_index, edge_attr)\n",
    "        x += x_out\n",
    "        edge_attr += edge_attr_out\n",
    "\n",
    "        x = F.relu(x)\n",
    "        edge_attr = F.relu(edge_attr)\n",
    "        x_out, edge_attr_out = self.graph_conv_4(x, edge_index, edge_attr)\n",
    "        x += x_out\n",
    "        edge_attr += edge_attr_out\n",
    "\n",
    "        x = self.linear_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_fixed_width(lst, precision=None):\n",
    "    lst = [round(l, precision) if isinstance(l, float) else l for l in lst]\n",
    "    return [f\"{l: <18}\" for l in lst]\n",
    "\n",
    "\n",
    "class Stats:\n",
    "    epoch: int\n",
    "    step: int\n",
    "    batch_size: int\n",
    "    echo: bool\n",
    "    total_loss: float\n",
    "    num_correct_preds: int\n",
    "    num_preds: int\n",
    "    num_correct_preds_missing: int\n",
    "    num_preds_missing: int\n",
    "    num_correct_preds_missing_valid: int\n",
    "    num_preds_missing_valid: int\n",
    "    start_time: float\n",
    "\n",
    "    def __init__(self, *, epoch=0, step=0, batch_size=1, filename=None, echo=True):\n",
    "        self.epoch = epoch\n",
    "        self.step = step\n",
    "        self.batch_size = batch_size\n",
    "        self.echo = echo\n",
    "        self.reset_parameters()\n",
    "\n",
    "        if filename:\n",
    "            self.filehandle = open(filename, \"wt\", newline=\"\")\n",
    "            self.writer = csv.DictWriter(self.filehandle, list(self.stats.keys()), dialect=\"unix\")\n",
    "            self.writer.writeheader()\n",
    "            atexit.register(self.filehandle.close)\n",
    "        else:\n",
    "            self.filehandle = None\n",
    "            self.writer = None\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.num_steps = 0\n",
    "        self.total_loss = 0\n",
    "        self.num_correct_preds = 0\n",
    "        self.num_preds = 0\n",
    "        self.num_correct_preds_missing = 0\n",
    "        self.num_preds_missing = 0\n",
    "        self.num_correct_preds_missing_valid = 0\n",
    "        self.num_preds_missing_valid = 0\n",
    "        self.start_time = time.perf_counter()\n",
    "\n",
    "    @property\n",
    "    def num_processed_examples(self):\n",
    "        return int(self.num_steps * self.batch_size)\n",
    "\n",
    "    @property\n",
    "    def header(self):\n",
    "        return \"\".join(to_fixed_width(self.stats.keys()))\n",
    "\n",
    "    @property\n",
    "    def row(self):\n",
    "        return \"\".join(to_fixed_width(self.stats.values(), 4))\n",
    "\n",
    "    @property\n",
    "    def stats(self):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return {\n",
    "                \"epoch\": self.epoch,\n",
    "                \"step\": self.step,\n",
    "                \"datapoint\": self.num_processed_examples,\n",
    "                \"avg_loss\": np.float64(1) * self.total_loss / self.num_steps,\n",
    "                \"accuracy\": np.float64(1) * self.num_correct_preds / self.num_preds,\n",
    "                \"accuracy_m\": np.float64(1) * self.num_correct_preds_missing / self.num_preds_missing,\n",
    "                \"accuracy_mv\": np.float64(1) * self.num_correct_preds_missing_valid / self.num_preds_missing_valid,\n",
    "                \"time_elapsed\": time.perf_counter() - self.start_time,\n",
    "            }\n",
    "\n",
    "    def write_header(self):\n",
    "        if self.echo:\n",
    "            print(self.header)\n",
    "        if self.writer is not None:\n",
    "            self.writer.writeheader()\n",
    "\n",
    "    def write_row(self):\n",
    "        if self.echo:\n",
    "            print(self.row)\n",
    "        if self.writer is not None:\n",
    "            self.writer.writerow(self.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch             step              datapoint         avg_loss          accuracy          accuracy_m        accuracy_mv       time_elapsed      \n",
      "0                 0                 64                0.0               0.0               0.0               0.0               0.0003            \n"
     ]
    }
   ],
   "source": [
    "stats = Stats(epoch=0, step=0, batch_size=64, filename=tempfile.NamedTemporaryFile().name, echo=True)\n",
    "stats.num_steps += 1\n",
    "stats.num_preds += 1\n",
    "stats.num_preds_missing += 1\n",
    "stats.num_preds_missing_valid += 1\n",
    "\n",
    "stats.write_header()\n",
    "stats.write_row()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "device = torch.device(\"cuda:1\")\n",
    "# device = \"cpu\"\n",
    "batch_size = 32\n",
    "num_features = 20\n",
    "adj_input_size = 2\n",
    "hidden_size = 128\n",
    "frac_present = 0.5\n",
    "frac_present_valid = frac_present\n",
    "info_size= 1024\n",
    "\n",
    "dataloaders = {\n",
    "    \"train_0\": DataLoader(protein_dataset_train_0, shuffle=True, num_workers=4, batch_size=batch_size, drop_last=True),\n",
    "    \"train_1\": DataLoader(protein_dataset_train_1, shuffle=True, num_workers=4, batch_size=batch_size, drop_last=True),\n",
    "    \"train_2\": None,\n",
    "    \"valid\": DataLoader(protein_dataset_valid[:128], shuffle=False, num_workers=4, batch_size=1, drop_last=False),\n",
    "    \"test\": DataLoader(protein_dataset_test[:128], shuffle=False, num_workers=4, batch_size=1, drop_last=False),\n",
    "\n",
    "    \"gfp_train\": DataLoader(protein_dataset_test[:64], shuffle=False, num_workers=0, batch_size=batch_size, drop_last=False),\n",
    "    \"gfp_valid\": DataLoader(protein_dataset_test[-32:], shuffle=False, num_workers=0, batch_size=batch_size, drop_last=False),\n",
    "}\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6380
    },
    "colab_type": "code",
    "id": "azxPKTSKwcE8",
    "outputId": "09ed9b8b-25b9-49ff-8d02-83a4ec0ecba1"
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def eval_net(net: nn.Module):\n",
    "    training = net.training\n",
    "    try:\n",
    "        net.train(False)\n",
    "        yield\n",
    "    finally:\n",
    "        net.train(training)\n",
    "\n",
    "\n",
    "def get_stats_on_missing(data, output):\n",
    "    mask = (data.x == num_features).squeeze()\n",
    "    output_missing = output[mask]\n",
    "    _, predicted_missing = torch.max(output_missing.data, 1)\n",
    "    return (predicted_missing == data.y[mask]).sum().item(), len(predicted_missing)\n",
    "\n",
    "\n",
    "def get_data_x(data, frac_present):\n",
    "    x = torch.where(\n",
    "        torch.rand(data.y.size(0), device=device) < frac_present,\n",
    "        data.y,\n",
    "        torch.ones(1, dtype=torch.long, device=device) * num_features,\n",
    "    )\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6380
    },
    "colab_type": "code",
    "id": "azxPKTSKwcE8",
    "outputId": "09ed9b8b-25b9-49ff-8d02-83a4ec0ecba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch             step              datapoint         avg_loss          accuracy          accuracy_m        accuracy_mv       time_elapsed      \n",
      "15                32                1024              2.2597            0.3961            0.0701            0.0941            19.589            \n",
      "31                64                1024              1.6332            0.5284            0.087             0.0902            19.6206           \n",
      "47                96                1024              1.5319            0.537             0.0912            0.0829            19.6919           \n",
      "63                128               1024              1.5044            0.5407            0.0958            0.0804            19.6593           \n",
      "79                160               1024              1.4944            0.5423            0.0984            0.0804            19.5653           \n",
      "95                192               1024              1.4799            0.546             0.1007            0.0822            19.6784           \n",
      "111               224               1024              1.4785            0.5467            0.1035            0.0804            19.7108           \n",
      "127               256               1024              1.4703            0.5479            0.1035            0.0817            19.7846           \n",
      "143               288               1024              1.4536            0.5542            0.1078            0.0806            19.7333           \n",
      "159               320               1024              1.4516            0.5539            0.1084            0.0802            19.7607           \n",
      "175               352               1024              1.459             0.5511            0.1097            0.0904            19.7582           \n",
      "191               384               1024              1.4543            0.5523            0.111             0.0896            19.8311           \n",
      "207               416               1024              1.4346            0.5589            0.116             0.0746            19.8428           \n",
      "223               448               1024              1.4366            0.5588            0.1178            0.0824            19.7132           \n",
      "239               480               1024              1.4352            0.5578            0.118             0.0715            19.8606           \n",
      "255               512               1024              1.4252            0.561             0.1222            0.0882            20.2625           \n",
      "271               544               1024              1.4234            0.5616            0.1244            0.0935            20.09             \n"
     ]
    }
   ],
   "source": [
    "net = Net(\n",
    "    x_input_size=num_features + 1, adj_input_size=adj_input_size, hidden_size=hidden_size, output_size=num_features\n",
    ")\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "net = net.train()\n",
    "stats = Stats(epoch=0, step=0, batch_size=batch_size, filename=NOTEBOOK_PATH.joinpath(\"training.log\"), echo=True)\n",
    "stats.write_header()\n",
    "for epoch in range(0, 100_000):\n",
    "    stats.epoch = epoch\n",
    "    for i, data in enumerate(dataloaders[\"gfp_train\"]):\n",
    "        stats.step += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = data.to(device)\n",
    "        if data.x is None:\n",
    "            data.x = get_data_x(data, frac_present)\n",
    "        output = net(data.x, data.edge_index, data.edge_attr if hasattr(data, \"edge_attr\") else None)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "\n",
    "        stats.total_loss += loss.detach().item()\n",
    "        stats.num_steps += 1\n",
    "\n",
    "        # Accuracy for all\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        stats.num_correct_preds += (predicted == data.y).sum().item()\n",
    "        stats.num_preds += len(predicted)\n",
    "\n",
    "        # Accuracy for missing only\n",
    "        num_correct, num_total = get_stats_on_missing(data, output)\n",
    "        stats.num_correct_preds_missing += num_correct\n",
    "        stats.num_preds_missing += num_total\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (stats.num_processed_examples % info_size) < batch_size:\n",
    "            for j, data in enumerate(dataloaders[\"gfp_valid\"]):\n",
    "                data = data.to(device)\n",
    "                if data.x is None:\n",
    "                    data.x = get_data_x(data, frac_present_valid)\n",
    "\n",
    "                with torch.no_grad() and eval_net(net):\n",
    "                    output = net(data.x, data.edge_index, data.edge_attr if hasattr(data, \"edge_attr\") else None)\n",
    "\n",
    "                num_correct, num_total = get_stats_on_missing(data, output)\n",
    "                stats.num_correct_preds_missing_valid += num_correct\n",
    "                stats.num_preds_missing_valid += num_total\n",
    "\n",
    "            stats.write_row()\n",
    "            stats.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(\n",
    "    x_input_size=num_features + 1, adj_input_size=adj_input_size, hidden_size=hidden_size, output_size=num_features\n",
    ")\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', verbose=True)\n",
    "\n",
    "net = net.train()\n",
    "stats = Stats(epoch=0, step=0, batch_size=batch_size, filename=NOTEBOOK_PATH.joinpath(\"training.log\"), echo=True)\n",
    "stats.write_header()\n",
    "for epoch in range(0, 10_000):\n",
    "    stats.epoch = epoch\n",
    "    \n",
    "    data_ref = next(iter(dataloaders[\"train_0\"]))\n",
    "    data_ref = data_ref.to(device)\n",
    "    \n",
    "    data_valid_ref = next(iter(dataloaders[\"valid\"]))\n",
    "    data_valid_ref = data_valid_ref.to(device)\n",
    "\n",
    "    for i in range(100000):\n",
    "        stats.step = i\n",
    "        \n",
    "        data = data_ref.clone()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if data.x is None:\n",
    "            data.x = get_data_x(data, frac_present)\n",
    "        output = net(data.x, data.edge_index, data.edge_attr if hasattr(data, \"edge_attr\") else None)\n",
    "        \n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "\n",
    "        stats.total_loss += loss.detach().item()\n",
    "        stats.num_steps += 1\n",
    "\n",
    "        # Accuracy for all\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        stats.num_correct_preds += (predicted == data.y).sum().item()\n",
    "        stats.num_preds += len(predicted)\n",
    "\n",
    "        # Accuracy for missing only\n",
    "        num_correct, num_total = get_stats_on_missing(data, output)\n",
    "        stats.num_correct_preds_missing += num_correct\n",
    "        stats.num_preds_missing += num_total\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "        if (stats.num_processed_examples % info_size) < batch_size:\n",
    "            data = data_valid_ref.clone()\n",
    "\n",
    "            if data.x is None:\n",
    "                data.x = get_data_x(data, frac_present_valid)\n",
    "\n",
    "            with torch.no_grad() and eval_net(net):\n",
    "                output = net(data.x, data.edge_index, data.edge_attr if hasattr(data, \"edge_attr\") else None)\n",
    "\n",
    "            num_correct, num_total = get_stats_on_missing(data, output)\n",
    "            stats.num_correct_preds_missing_valid += num_correct\n",
    "            stats.num_preds_missing_valid += num_total\n",
    "\n",
    "            scheduler.step(stats.stats['accuracy'])\n",
    "            stats.write_row()\n",
    "            stats.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1fXJz1l9HvhW",
    "u9q4iTifHvhl"
   ],
   "name": "Copy of 2019-03-30-sudoku-4xEdgeConv-09862+.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
