{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_gnhLD0HvhU",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fXJz1l9HvhW"
   },
   "source": [
    "### Install `pytorch_geometric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "hIbvKDk_HvhX",
    "outputId": "349cb6d3-ffbc-4550-d9dd-dd6288ed5356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-scatter\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/83/67eeea00c2db1959e2ff95d8680dbd756977bfab254bda8658f09dc3bc11/torch_scatter-1.1.2.tar.gz\n",
      "Building wheels for collected packages: torch-scatter\n",
      "  Building wheel for torch-scatter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a8/41/ef/1a52be728eedba23aa6d39b0bd6e9b533cb7ff572e967a6a43\n",
      "Successfully built torch-scatter\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "tw-y9MmIHvha",
    "outputId": "0666864a-5017-40a1-a77c-d23388eb0b1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-sparse\n",
      "  Downloading https://files.pythonhosted.org/packages/73/72/e374662f6f47d9ac0e082a6d5c18d14e15c52863e89c6bc6957a0d2ed026/torch_sparse-0.2.4.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse) (1.14.6)\n",
      "Building wheels for collected packages: torch-sparse\n",
      "  Building wheel for torch-sparse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/14/8f/77/7be0a2e42c5d1a6257b71edf5dc42f9e148d2137b66d869ee0\n",
      "Successfully built torch-sparse\n",
      "Installing collected packages: torch-sparse\n",
      "Successfully installed torch-sparse-0.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "n3SMtaYHHvhc",
    "outputId": "6250a80b-b32d-4fc8-cf90-ad0507e83ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-cluster\n",
      "  Downloading https://files.pythonhosted.org/packages/32/c8/9b3af10be647326dd807bb2fe7ced8ae4c3fd74178dba884621749afc4d7/torch_cluster-1.2.4.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from torch-cluster) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-cluster) (1.14.6)\n",
      "Building wheels for collected packages: torch-cluster\n",
      "  Building wheel for torch-cluster (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/5b/54/4a/ae0125851936c2b5ea13dddb1ab4b103b9f71e0f3211a6c4e1\n",
      "Successfully built torch-cluster\n",
      "Installing collected packages: torch-cluster\n",
      "Successfully installed torch-cluster-1.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "4AQ_A4XLHvhg",
    "outputId": "601b9d38-8291-431d-a90b-046bb233469a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-spline-conv\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/6d/b34721af4bb907814a41a1e0e5e426c97aee644efef6877ed112bfb3d81c/torch_spline_conv-1.0.6.tar.gz\n",
      "Building wheels for collected packages: torch-spline-conv\n",
      "  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f6/e2/d7/d87645a1e1d34b51633b7fd13f020b618c0ee84000b4f4085c\n",
      "Successfully built torch-spline-conv\n",
      "Installing collected packages: torch-spline-conv\n",
      "Successfully installed torch-spline-conv-1.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-spline-conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "colab_type": "code",
    "id": "uk6E9JP_Hvhj",
    "outputId": "5a986a43-4d09-41ab-efb6-9fb1cab56cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/4d/06dd0d277bf82ff4e1888f73b4c522c35f017505acacc25ecc95befaac6e/torch_geometric-1.1.0.tar.gz (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 2.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.14.6)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.1.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.20.3)\n",
      "Collecting plyfile (from torch-geometric)\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/82/d4069cbb49954d44087c37ff616cb423d3e2c0dd276378cdb4af3e3ef2ee/plyfile-0.7.tar.gz\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.0)\n",
      "Collecting rdflib (from torch-geometric)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
      "\u001b[K    100% |████████████████████████████████| 348kB 24.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.5.3)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.3.1)\n",
      "Collecting isodate (from rdflib->torch-geometric)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 14.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas->torch-geometric) (1.11.0)\n",
      "Building wheels for collected packages: torch-geometric, plyfile\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3e/c1/87/daa10b8fa568fb9a84ff1d9c24582af1ecc7e797e5e69ff58e\n",
      "  Building wheel for plyfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/91/3e/ee/e5630ef0fd53cedaa6e911ba27e8b40fff034388d1f264bb92\n",
      "Successfully built torch-geometric plyfile\n",
      "Installing collected packages: plyfile, isodate, rdflib, torch-geometric\n",
      "Successfully installed isodate-0.6.0 plyfile-0.7 rdflib-4.2.2 torch-geometric-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9q4iTifHvhl"
   },
   "source": [
    "### Install `proteinsolver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "colab_type": "code",
    "id": "HCWw1IgNHvhm",
    "outputId": "fcb1a352-1ca8-4708-c3af-11bb0f9a3c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://gitlab%2Bdeploy-token-52110:****@gitlab.com/ostrokach/proteinsolver.git\n",
      "  Cloning https://gitlab%2Bdeploy-token-52110:****@gitlab.com/ostrokach/proteinsolver.git to /tmp/pip-req-build-u2bdbexg\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: proteinsolver\n",
      "  Building wheel for proteinsolver (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-6rtg4lok/wheels/fb/05/26/209b40c24fc617333b311553f1c37610fb9b3475d67811bfcc\n",
      "Successfully built proteinsolver\n",
      "Installing collected packages: proteinsolver\n",
      "Successfully installed proteinsolver-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://gitlab.com/ostrokach/proteinsolver.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eoBMUoW2Hvhp",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "EmCKag-5MCiB",
    "outputId": "40059204-55bf-4285-eb0a-34098e980c40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  2 18:09:45 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbKxMUZWHvhq"
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "rczy7pPiHvhs",
    "outputId": "8e4673d7-4cc7-4f8e-ffc4-de94302e5fab"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-26ce5a0cde66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mproteinsolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproteinsolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'proteinsolver'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import proteinsolver\n",
    "import proteinsolver.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fFnAyUOHvhv"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsEY3dtLHvhy",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "TWxyIM2IPf_U",
    "outputId": "6bff5c80-de1c-4769-9efa-542f6978bc57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
       "        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "        90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(0, 100)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "vLUXa4JKPgJc",
    "outputId": "fd9e2263-e45d-4008-bffe-1d99b4349805"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
       "        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "        90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(t >= 10, t, torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZP3bLDO9PgT9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TYp6L0er89A"
   },
   "source": [
    "## `SudokuDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5b-O9kCr89B"
   },
   "outputs": [],
   "source": [
    "sudoku_dataset_train = proteinsolver.datasets.SudokuDataset(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "3GWm0x_Zr89D",
    "outputId": "da4602e1-0ba9-4eec-bb89-e972689c1b45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 1701], label=[1], x=[81, 1], y=[81, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sudoku_dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86nQhmhBr89I"
   },
   "outputs": [],
   "source": [
    "sudoku_dataset_valid = proteinsolver.datasets.SudokuDataset(\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "XRpLbeGzr89L",
    "outputId": "b778d595-316f-4c22-bf5a-2c4f7828a310"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 1701], label=[1], x=[81, 1], y=[81, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sudoku_dataset_valid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6W1sfkUHr89O"
   },
   "source": [
    "## `TUDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lOt0mVyir89P"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "Ctf7-3yYr89S",
    "outputId": "9ab47d4d-5cad-4f36-b32c-0f7b912d0e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ls11-www.cs.uni-dortmund.de/people/morris/graphkerneldatasets/ENZYMES.zip\n",
      "Extracting /tmp/ENZYMES/ENZYMES.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "tu_dataset = TUDataset(root=tempfile.gettempdir() + '/ENZYMES', name='ENZYMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "tLtEuvRwr89Y",
    "outputId": "ca22011f-8035-4c70-c722-4ed39b1bdc17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 168], x=[37, 3], y=[1])"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3kWuRaxr89h"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v01yrXOncfJJ"
   },
   "outputs": [],
   "source": [
    "# !pip install -U git+https://github.com/rusty1s/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xGLXgqYlr89j"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch_geometric.nn import GCNConv, ChebConv, GATConv, EdgeConv\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72WWaQOXJDdr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import scatter_\n",
    "\n",
    "from torch_geometric.nn.inits import reset\n",
    "\n",
    "\n",
    "class EdgeConvMod(torch.nn.Module):\n",
    "    def __init__(self, nn, aggr='max'):\n",
    "        super().__init__()\n",
    "        self.nn = nn\n",
    "        self.aggr = aggr\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "\n",
    "    def forward(self, x, edge_index, out_prev=None):\n",
    "        \"\"\"\"\"\"\n",
    "        row, col = edge_index\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "\n",
    "        if out_prev is None:\n",
    "            out = torch.cat([x[row], x[col] - x[row]], dim=1)\n",
    "        else:\n",
    "            out = torch.cat([out_prev, x[row], x[col] - x[row]], dim=1)\n",
    "        out = self.nn(out)\n",
    "        x = scatter_(self.aggr, out, row, dim_size=x.size(0))\n",
    "\n",
    "        return x, out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(nn={})'.format(self.__class__.__name__, self.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDrlxheizsKg"
   },
   "outputs": [],
   "source": [
    "class EdgeConvBatch(nn.Module):\n",
    "    \n",
    "    def __init__(self, gnn, hidden_size, batch_norm=True, nonlin=False, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gnn = gnn\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.x_batch_norm = nn.BatchNorm1d(hidden_size) if batch_norm else None\n",
    "        self.adj_batch_norm = nn.BatchNorm1d(hidden_size) if batch_norm else None\n",
    "\n",
    "        self.x_nonlin = nn.ReLU() if nonlin else None\n",
    "        self.adj_nonlin = nn.ReLU() if nonlin else None\n",
    "\n",
    "        self.x_dropout = nn.Dropout(dropout) if dropout else None\n",
    "        self.adj_dropout = nn.Dropout(dropout) if dropout else None\n",
    "\n",
    "    def forward(self, x_batch, indices, adj_batch=None):\n",
    "        x_lst_out = []\n",
    "        adj_lst_out = []\n",
    "        \n",
    "        for i in range(x_batch.size(0)):\n",
    "            x_out, adj_out = self.gnn(\n",
    "                x_batch[i].transpose(-2, -1),\n",
    "                indices,\n",
    "                adj_batch[i].transpose(-2, -1) if adj_batch is not None else None,\n",
    "            )\n",
    "            x_lst_out.append(x_out.transpose(-2, -1))\n",
    "            adj_lst_out.append(adj_out.transpose(-2, -1))\n",
    "\n",
    "        x_batch_out = torch.stack(x_lst_out, dim=0)\n",
    "        adj_batch_out = torch.stack(adj_lst_out, dim=0)\n",
    "\n",
    "        if self.x_batch_norm is not None:\n",
    "            x_batch_out = self.x_batch_norm(x_batch_out)\n",
    "            adj_batch_out = self.adj_batch_norm(adj_batch_out)\n",
    "\n",
    "        if self.x_nonlin is not None:\n",
    "            x_batch_out = self.x_nonlin(x_batch_out)\n",
    "            adj_batch_out = self.adj_nonlin(adj_batch_out)\n",
    "\n",
    "        if self.x_dropout is not None:\n",
    "            x_batch_out = self.x_dropout(x_batch_out)\n",
    "            adj_batch_out = self.adj_dropout(adj_batch_out)\n",
    "            \n",
    "        return x_batch_out, adj_batch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNCaSlrFDGA6"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    # Make all hiddens 64\n",
    "    # Try relu instead of elu in GraphConv\n",
    "    # Try normalizing sum by the number of nz in that dim\n",
    "    # Try summing over both row and column\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden = 128\n",
    "        \n",
    "        self.embed = nn.Embedding(10, hidden)\n",
    "        self.norm_1 = nn.BatchNorm1d(hidden)\n",
    "\n",
    "        mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden, 2 * hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden, hidden),\n",
    "#             nn.ReLU(),\n",
    "        )\n",
    "        gnn = EdgeConvMod(nn=mlp, aggr=\"add\")\n",
    "        self.graph_conv_1 = EdgeConvBatch(gnn, hidden, batch_norm=True, nonlin=False, dropout=0.2)\n",
    "        \n",
    "        mlp = nn.Sequential(\n",
    "            nn.Linear(3 * hidden, 2 * hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden, hidden),\n",
    "#             nn.ReLU(),\n",
    "        )\n",
    "        gnn = EdgeConvMod(nn=mlp, aggr=\"add\")\n",
    "        self.graph_conv_2 = EdgeConvBatch(gnn, hidden, batch_norm=True, nonlin=False, dropout=0.2)\n",
    "\n",
    "        mlp = nn.Sequential(\n",
    "            nn.Linear(3 * hidden, 2 * hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden, hidden),\n",
    "#             nn.ReLU(),\n",
    "        )\n",
    "        gnn = EdgeConvMod(nn=mlp, aggr=\"add\")  # Try \"max\" here\n",
    "        self.graph_conv_3 = EdgeConvBatch(gnn, hidden, batch_norm=True, nonlin=False, dropout=0.2)\n",
    "\n",
    "        mlp = nn.Sequential(\n",
    "            nn.Linear(3 * hidden, 2 * hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * hidden, hidden),\n",
    "#             nn.ReLU(),\n",
    "        )\n",
    "        gnn = EdgeConvMod(nn=mlp, aggr=\"add\")  # Try \"max\" here\n",
    "        self.graph_conv_4 = EdgeConvBatch(gnn, hidden, batch_norm=True, nonlin=False, dropout=0.2)\n",
    "\n",
    "        self.linear_out = nn.Conv1d(hidden, 9, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        x_lst = []\n",
    "        for x, indices in batch:\n",
    "            x = self.embed(x)\n",
    "            x_lst.append(x)\n",
    "        x_batch = torch.stack(x_lst, dim=0).transpose(-2, -1)\n",
    "        x_batch = self.norm_1(x_batch)\n",
    "            \n",
    "        indices = batch[0][1]\n",
    "        indices, _ = remove_self_loops(indices)  # We should remove self loops in this case!\n",
    "\n",
    "        x_batch_out, adj_batch_out = self.graph_conv_1(x_batch, indices)\n",
    "        x_batch += x_batch_out\n",
    "        adj_batch = adj_batch_out\n",
    "        \n",
    "        x_batch = F.relu(x_batch)\n",
    "        adj_batch = F.relu(adj_batch)\n",
    "        \n",
    "        x_batch_out, adj_batch_out = self.graph_conv_2(x_batch, indices, adj_batch)\n",
    "        x_batch += x_batch_out\n",
    "        adj_batch += adj_batch_out\n",
    "\n",
    "        x_batch = F.relu(x_batch)\n",
    "        adj_batch = F.relu(adj_batch)\n",
    "\n",
    "        x_batch_out, adj_batch_out = self.graph_conv_3(x_batch, indices, adj_batch)\n",
    "        x_batch += x_batch_out\n",
    "        adj_batch += adj_batch_out\n",
    "\n",
    "        x_batch = F.relu(x_batch)\n",
    "        adj_batch = F.relu(adj_batch)\n",
    "\n",
    "        x_batch_out, adj_batch_out = self.graph_conv_4(x_batch, indices, adj_batch)\n",
    "        x_batch += x_batch_out\n",
    "        adj_batch += adj_batch_out\n",
    "\n",
    "        y_lst = []\n",
    "        for i in range(x_batch.size(0)):\n",
    "            x = self.linear_out(x_batch[i].unsqueeze(0)).squeeze()\n",
    "            x = x.transpose(0, 1)\n",
    "            y_lst.append(x)\n",
    "        \n",
    "        return y_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6380
    },
    "colab_type": "code",
    "id": "azxPKTSKwcE8",
    "outputId": "09ed9b8b-25b9-49ff-8d02-83a4ec0ecba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "epoch: 0     step: 0         loss: 2.3037    acc: 0.1554   accm: 0.1247   accmv: 0.0998   time: 6.223     \n",
      "epoch: 0     step: 2048      loss: 1.4446    acc: 0.5023   accm: 0.2149   accmv: 0.3556   time: 68.310    \n",
      "epoch: 0     step: 4096      loss: 1.1444    acc: 0.6409   accm: 0.3839   accmv: 0.5106   time: 67.819    \n",
      "epoch: 0     step: 6016      loss: 0.9718    acc: 0.7011   accm: 0.4870   accmv: 0.5577   time: 62.559    \n",
      "epoch: 0     step: 8064      loss: 0.7927    acc: 0.7364   accm: 0.5477   accmv: 0.6134   time: 67.538    \n",
      "epoch: 0     step: 10112     loss: 0.6615    acc: 0.7574   accm: 0.5838   accmv: 0.6816   time: 67.642    \n",
      "epoch: 0     step: 12032     loss: 0.5820    acc: 0.7714   accm: 0.6077   accmv: 0.7070   time: 63.738    \n",
      "epoch: 0     step: 14080     loss: 0.5388    acc: 0.7785   accm: 0.6198   accmv: 0.7260   time: 67.808    \n",
      "epoch: 0     step: 16000     loss: 0.5104    acc: 0.7836   accm: 0.6285   accmv: 0.7327   time: 63.806    \n",
      "epoch: 0     step: 18048     loss: 0.4876    acc: 0.7902   accm: 0.6397   accmv: 0.7406   time: 67.910    \n",
      "epoch: 0     step: 20096     loss: 0.4745    acc: 0.7936   accm: 0.6454   accmv: 0.7464   time: 67.837    \n",
      "epoch: 0     step: 22016     loss: 0.4613    acc: 0.7986   accm: 0.6542   accmv: 0.7561   time: 63.832    \n",
      "epoch: 0     step: 24064     loss: 0.4516    acc: 0.8015   accm: 0.6595   accmv: 0.7662   time: 67.946    \n",
      "epoch: 0     step: 26112     loss: 0.4417    acc: 0.8046   accm: 0.6647   accmv: 0.7589   time: 67.385    \n",
      "epoch: 0     step: 28032     loss: 0.4325    acc: 0.8091   accm: 0.6720   accmv: 0.7695   time: 62.705    \n",
      "epoch: 0     step: 30080     loss: 0.4268    acc: 0.8117   accm: 0.6768   accmv: 0.7635   time: 67.444    \n",
      "epoch: 0     step: 32000     loss: 0.4167    acc: 0.8166   accm: 0.6851   accmv: 0.7773   time: 63.536    \n",
      "epoch: 0     step: 34048     loss: 0.4102    acc: 0.8194   accm: 0.6899   accmv: 0.7756   time: 67.589    \n",
      "epoch: 0     step: 36096     loss: 0.4062    acc: 0.8224   accm: 0.6953   accmv: 0.7769   time: 67.441    \n",
      "epoch: 0     step: 38016     loss: 0.4027    acc: 0.8233   accm: 0.6970   accmv: 0.7781   time: 63.599    \n",
      "epoch: 0     step: 40064     loss: 0.3907    acc: 0.8287   accm: 0.7058   accmv: 0.7829   time: 67.574    \n",
      "epoch: 0     step: 42112     loss: 0.3850    acc: 0.8321   accm: 0.7117   accmv: 0.7869   time: 67.474    \n",
      "epoch: 0     step: 44032     loss: 0.3827    acc: 0.8339   accm: 0.7149   accmv: 0.7962   time: 63.506    \n",
      "epoch: 0     step: 46080     loss: 0.3794    acc: 0.8352   accm: 0.7173   accmv: 0.7897   time: 67.717    \n",
      "epoch: 0     step: 48000     loss: 0.3702    acc: 0.8409   accm: 0.7266   accmv: 0.7985   time: 62.948    \n",
      "epoch: 0     step: 50048     loss: 0.3646    acc: 0.8437   accm: 0.7315   accmv: 0.7987   time: 67.759    \n",
      "epoch: 0     step: 52096     loss: 0.3618    acc: 0.8449   accm: 0.7340   accmv: 0.7968   time: 67.768    \n",
      "epoch: 0     step: 54016     loss: 0.3556    acc: 0.8487   accm: 0.7402   accmv: 0.8035   time: 63.604    \n",
      "epoch: 0     step: 56064     loss: 0.3512    acc: 0.8512   accm: 0.7445   accmv: 0.8027   time: 67.674    \n",
      "epoch: 0     step: 58112     loss: 0.3467    acc: 0.8536   accm: 0.7487   accmv: 0.8103   time: 67.646    \n",
      "epoch: 0     step: 60032     loss: 0.3419    acc: 0.8570   accm: 0.7544   accmv: 0.8095   time: 63.671    \n",
      "epoch: 0     step: 62080     loss: 0.3381    acc: 0.8587   accm: 0.7575   accmv: 0.8126   time: 67.713    \n",
      "epoch: 0     step: 64000     loss: 0.3329    acc: 0.8620   accm: 0.7632   accmv: 0.8174   time: 64.014    \n",
      "epoch: 0     step: 66048     loss: 0.3286    acc: 0.8640   accm: 0.7665   accmv: 0.8182   time: 68.097    \n",
      "epoch: 0     step: 68096     loss: 0.3254    acc: 0.8655   accm: 0.7691   accmv: 0.8206   time: 67.828    \n",
      "epoch: 0     step: 70016     loss: 0.3195    acc: 0.8687   accm: 0.7745   accmv: 0.8231   time: 62.678    \n",
      "epoch: 0     step: 72064     loss: 0.3178    acc: 0.8698   accm: 0.7764   accmv: 0.8284   time: 67.818    \n",
      "epoch: 0     step: 74112     loss: 0.3105    acc: 0.8733   accm: 0.7826   accmv: 0.8279   time: 67.573    \n",
      "epoch: 0     step: 76032     loss: 0.3077    acc: 0.8762   accm: 0.7875   accmv: 0.8290   time: 63.837    \n",
      "epoch: 0     step: 78080     loss: 0.3025    acc: 0.8782   accm: 0.7909   accmv: 0.8353   time: 67.827    \n",
      "epoch: 0     step: 80000     loss: 0.2972    acc: 0.8804   accm: 0.7948   accmv: 0.8362   time: 63.576    \n",
      "epoch: 0     step: 82048     loss: 0.2908    acc: 0.8840   accm: 0.8010   accmv: 0.8403   time: 67.540    \n",
      "epoch: 0     step: 84096     loss: 0.2884    acc: 0.8854   accm: 0.8032   accmv: 0.8453   time: 67.816    \n",
      "epoch: 0     step: 86016     loss: 0.2828    acc: 0.8884   accm: 0.8085   accmv: 0.8469   time: 63.684    \n",
      "epoch: 0     step: 88064     loss: 0.2795    acc: 0.8895   accm: 0.8104   accmv: 0.8507   time: 68.136    \n",
      "epoch: 0     step: 90112     loss: 0.2723    acc: 0.8932   accm: 0.8166   accmv: 0.8582   time: 67.604    \n",
      "epoch: 0     step: 92032     loss: 0.2659    acc: 0.8961   accm: 0.8217   accmv: 0.8564   time: 63.000    \n",
      "epoch: 0     step: 94080     loss: 0.2622    acc: 0.8984   accm: 0.8256   accmv: 0.8635   time: 67.553    \n",
      "epoch: 0     step: 96000     loss: 0.2560    acc: 0.9015   accm: 0.8308   accmv: 0.8726   time: 63.587    \n",
      "epoch: 0     step: 98048     loss: 0.2491    acc: 0.9042   accm: 0.8354   accmv: 0.8789   time: 67.385    \n",
      "epoch: 0     step: 100096    loss: 0.2470    acc: 0.9058   accm: 0.8384   accmv: 0.8821   time: 67.505    \n",
      "epoch: 0     step: 102016    loss: 0.2320    acc: 0.9126   accm: 0.8500   accmv: 0.8881   time: 63.669    \n",
      "epoch: 0     step: 104064    loss: 0.2278    acc: 0.9140   accm: 0.8525   accmv: 0.8952   time: 67.746    \n",
      "epoch: 0     step: 106112    loss: 0.2228    acc: 0.9163   accm: 0.8564   accmv: 0.8967   time: 67.804    \n",
      "epoch: 0     step: 108032    loss: 0.2141    acc: 0.9202   accm: 0.8629   accmv: 0.9022   time: 63.699    \n",
      "epoch: 0     step: 110080    loss: 0.2072    acc: 0.9229   accm: 0.8677   accmv: 0.9070   time: 67.805    \n",
      "epoch: 0     step: 112000    loss: 0.1995    acc: 0.9254   accm: 0.8720   accmv: 0.9133   time: 62.827    \n",
      "epoch: 0     step: 114048    loss: 0.1936    acc: 0.9280   accm: 0.8765   accmv: 0.9184   time: 67.654    \n",
      "epoch: 0     step: 116096    loss: 0.1830    acc: 0.9329   accm: 0.8846   accmv: 0.9202   time: 67.857    \n",
      "epoch: 0     step: 118016    loss: 0.1818    acc: 0.9328   accm: 0.8846   accmv: 0.9234   time: 63.734    \n",
      "epoch: 0     step: 120064    loss: 0.1729    acc: 0.9369   accm: 0.8916   accmv: 0.9252   time: 67.324    \n",
      "epoch: 0     step: 122112    loss: 0.1690    acc: 0.9377   accm: 0.8931   accmv: 0.9277   time: 67.739    \n",
      "epoch: 0     step: 124032    loss: 0.1643    acc: 0.9409   accm: 0.8985   accmv: 0.9318   time: 64.112    \n",
      "epoch: 0     step: 126080    loss: 0.1620    acc: 0.9408   accm: 0.8985   accmv: 0.9353   time: 67.775    \n",
      "epoch: 0     step: 128000    loss: 0.1548    acc: 0.9435   accm: 0.9031   accmv: 0.9350   time: 63.601    \n",
      "epoch: 0     step: 130048    loss: 0.1477    acc: 0.9463   accm: 0.9078   accmv: 0.9358   time: 67.602    \n",
      "epoch: 0     step: 132096    loss: 0.1477    acc: 0.9460   accm: 0.9073   accmv: 0.9383   time: 68.036    \n",
      "epoch: 0     step: 134016    loss: 0.1440    acc: 0.9471   accm: 0.9093   accmv: 0.9398   time: 62.581    \n",
      "epoch: 0     step: 136064    loss: 0.1389    acc: 0.9491   accm: 0.9126   accmv: 0.9428   time: 67.864    \n",
      "epoch: 0     step: 138112    loss: 0.1365    acc: 0.9503   accm: 0.9146   accmv: 0.9461   time: 67.653    \n",
      "epoch: 0     step: 140032    loss: 0.1344    acc: 0.9505   accm: 0.9149   accmv: 0.9443   time: 63.593    \n",
      "epoch: 0     step: 142080    loss: 0.1320    acc: 0.9517   accm: 0.9171   accmv: 0.9444   time: 67.545    \n",
      "epoch: 0     step: 144000    loss: 0.1277    acc: 0.9531   accm: 0.9195   accmv: 0.9456   time: 63.866    \n",
      "epoch: 0     step: 146048    loss: 0.1252    acc: 0.9540   accm: 0.9210   accmv: 0.9469   time: 67.341    \n",
      "epoch: 0     step: 148096    loss: 0.1228    acc: 0.9551   accm: 0.9230   accmv: 0.9494   time: 67.691    \n",
      "epoch: 0     step: 150016    loss: 0.1179    acc: 0.9571   accm: 0.9263   accmv: 0.9507   time: 63.894    \n",
      "epoch: 0     step: 152064    loss: 0.1188    acc: 0.9568   accm: 0.9259   accmv: 0.9517   time: 67.263    \n",
      "epoch: 0     step: 154112    loss: 0.1154    acc: 0.9574   accm: 0.9270   accmv: 0.9489   time: 67.298    \n",
      "epoch: 0     step: 156032    loss: 0.1166    acc: 0.9567   accm: 0.9258   accmv: 0.9501   time: 62.861    \n",
      "epoch: 0     step: 158080    loss: 0.1128    acc: 0.9588   accm: 0.9292   accmv: 0.9519   time: 67.683    \n",
      "epoch: 0     step: 160000    loss: 0.1093    acc: 0.9599   accm: 0.9312   accmv: 0.9559   time: 63.305    \n",
      "epoch: 0     step: 162048    loss: 0.1091    acc: 0.9601   accm: 0.9316   accmv: 0.9542   time: 67.436    \n",
      "epoch: 0     step: 164096    loss: 0.1076    acc: 0.9605   accm: 0.9322   accmv: 0.9529   time: 67.755    \n",
      "epoch: 0     step: 166016    loss: 0.1043    acc: 0.9618   accm: 0.9345   accmv: 0.9547   time: 63.438    \n",
      "epoch: 0     step: 168064    loss: 0.1041    acc: 0.9619   accm: 0.9346   accmv: 0.9582   time: 67.380    \n",
      "epoch: 0     step: 170112    loss: 0.1011    acc: 0.9626   accm: 0.9359   accmv: 0.9551   time: 67.681    \n",
      "epoch: 0     step: 172032    loss: 0.0991    acc: 0.9640   accm: 0.9382   accmv: 0.9569   time: 63.488    \n",
      "epoch: 0     step: 174080    loss: 0.0988    acc: 0.9638   accm: 0.9379   accmv: 0.9589   time: 67.289    \n",
      "epoch: 0     step: 176000    loss: 0.0968    acc: 0.9644   accm: 0.9389   accmv: 0.9592   time: 62.562    \n",
      "epoch: 0     step: 178048    loss: 0.0954    acc: 0.9649   accm: 0.9397   accmv: 0.9595   time: 67.528    \n",
      "epoch: 0     step: 180096    loss: 0.0937    acc: 0.9652   accm: 0.9402   accmv: 0.9622   time: 67.403    \n",
      "epoch: 0     step: 182016    loss: 0.0929    acc: 0.9656   accm: 0.9409   accmv: 0.9622   time: 63.396    \n",
      "epoch: 0     step: 184064    loss: 0.0919    acc: 0.9667   accm: 0.9428   accmv: 0.9642   time: 67.139    \n",
      "epoch: 0     step: 186112    loss: 0.0904    acc: 0.9666   accm: 0.9427   accmv: 0.9637   time: 67.202    \n",
      "epoch: 0     step: 188032    loss: 0.0871    acc: 0.9679   accm: 0.9450   accmv: 0.9640   time: 63.283    \n",
      "epoch: 0     step: 190080    loss: 0.0859    acc: 0.9686   accm: 0.9461   accmv: 0.9658   time: 67.123    \n",
      "epoch: 0     step: 192000    loss: 0.0867    acc: 0.9680   accm: 0.9450   accmv: 0.9643   time: 63.330    \n",
      "epoch: 0     step: 194048    loss: 0.0842    acc: 0.9693   accm: 0.9473   accmv: 0.9650   time: 67.431    \n",
      "epoch: 0     step: 196096    loss: 0.0878    acc: 0.9676   accm: 0.9444   accmv: 0.9667   time: 67.330    \n",
      "epoch: 0     step: 198016    loss: 0.0848    acc: 0.9691   accm: 0.9470   accmv: 0.9668   time: 62.596    \n",
      "epoch: 0     step: 200064    loss: 0.0816    acc: 0.9700   accm: 0.9486   accmv: 0.9683   time: 67.444    \n",
      "epoch: 0     step: 202112    loss: 0.0804    acc: 0.9706   accm: 0.9495   accmv: 0.9683   time: 67.809    \n",
      "epoch: 0     step: 204032    loss: 0.0823    acc: 0.9692   accm: 0.9472   accmv: 0.9672   time: 63.912    \n",
      "epoch: 0     step: 206080    loss: 0.0799    acc: 0.9704   accm: 0.9492   accmv: 0.9687   time: 67.857    \n",
      "epoch: 0     step: 208000    loss: 0.0768    acc: 0.9719   accm: 0.9518   accmv: 0.9687   time: 63.656    \n",
      "epoch: 0     step: 210048    loss: 0.0776    acc: 0.9716   accm: 0.9513   accmv: 0.9703   time: 67.726    \n",
      "epoch: 0     step: 212096    loss: 0.0757    acc: 0.9721   accm: 0.9521   accmv: 0.9698   time: 67.732    \n",
      "epoch: 0     step: 214016    loss: 0.0768    acc: 0.9723   accm: 0.9525   accmv: 0.9713   time: 63.586    \n",
      "epoch: 0     step: 216064    loss: 0.0734    acc: 0.9733   accm: 0.9542   accmv: 0.9682   time: 67.610    \n",
      "epoch: 0     step: 218112    loss: 0.0720    acc: 0.9738   accm: 0.9550   accmv: 0.9713   time: 67.669    \n",
      "epoch: 0     step: 220032    loss: 0.0723    acc: 0.9731   accm: 0.9539   accmv: 0.9726   time: 62.544    \n",
      "epoch: 0     step: 222080    loss: 0.0728    acc: 0.9728   accm: 0.9534   accmv: 0.9703   time: 67.597    \n",
      "epoch: 0     step: 224000    loss: 0.0695    acc: 0.9747   accm: 0.9566   accmv: 0.9720   time: 63.659    \n",
      "epoch: 0     step: 226048    loss: 0.0701    acc: 0.9743   accm: 0.9558   accmv: 0.9701   time: 67.283    \n",
      "epoch: 0     step: 228096    loss: 0.0705    acc: 0.9739   accm: 0.9552   accmv: 0.9745   time: 67.372    \n",
      "epoch: 0     step: 230016    loss: 0.0703    acc: 0.9743   accm: 0.9559   accmv: 0.9708   time: 63.409    \n",
      "epoch: 0     step: 232064    loss: 0.0676    acc: 0.9750   accm: 0.9570   accmv: 0.9730   time: 67.497    \n",
      "epoch: 0     step: 234112    loss: 0.0690    acc: 0.9746   accm: 0.9565   accmv: 0.9710   time: 67.690    \n",
      "epoch: 0     step: 236032    loss: 0.0694    acc: 0.9747   accm: 0.9565   accmv: 0.9723   time: 63.252    \n",
      "epoch: 0     step: 238080    loss: 0.0667    acc: 0.9754   accm: 0.9578   accmv: 0.9743   time: 67.863    \n",
      "epoch: 0     step: 240000    loss: 0.0677    acc: 0.9748   accm: 0.9567   accmv: 0.9725   time: 62.283    \n",
      "epoch: 0     step: 242048    loss: 0.0658    acc: 0.9759   accm: 0.9587   accmv: 0.9728   time: 67.312    \n",
      "epoch: 0     step: 244096    loss: 0.0644    acc: 0.9765   accm: 0.9597   accmv: 0.9743   time: 67.367    \n",
      "epoch: 0     step: 246016    loss: 0.0630    acc: 0.9772   accm: 0.9609   accmv: 0.9756   time: 63.627    \n",
      "epoch: 0     step: 248064    loss: 0.0621    acc: 0.9774   accm: 0.9611   accmv: 0.9748   time: 67.276    \n",
      "epoch: 0     step: 250112    loss: 0.0650    acc: 0.9759   accm: 0.9585   accmv: 0.9746   time: 67.220    \n",
      "epoch: 0     step: 252032    loss: 0.0624    acc: 0.9773   accm: 0.9610   accmv: 0.9730   time: 63.461    \n",
      "epoch: 0     step: 254080    loss: 0.0615    acc: 0.9775   accm: 0.9614   accmv: 0.9746   time: 67.184    \n",
      "epoch: 0     step: 256000    loss: 0.0628    acc: 0.9767   accm: 0.9599   accmv: 0.9735   time: 63.341    \n",
      "epoch: 0     step: 258048    loss: 0.0592    acc: 0.9784   accm: 0.9628   accmv: 0.9768   time: 67.480    \n",
      "epoch: 0     step: 260096    loss: 0.0586    acc: 0.9786   accm: 0.9632   accmv: 0.9756   time: 67.394    \n",
      "epoch: 0     step: 262016    loss: 0.0596    acc: 0.9783   accm: 0.9627   accmv: 0.9738   time: 62.836    \n",
      "epoch: 0     step: 264064    loss: 0.0599    acc: 0.9777   accm: 0.9618   accmv: 0.9751   time: 67.899    \n",
      "epoch: 0     step: 266112    loss: 0.0580    acc: 0.9788   accm: 0.9635   accmv: 0.9755   time: 67.538    \n",
      "epoch: 0     step: 268032    loss: 0.0566    acc: 0.9790   accm: 0.9640   accmv: 0.9769   time: 63.262    \n",
      "epoch: 0     step: 270080    loss: 0.0578    acc: 0.9787   accm: 0.9635   accmv: 0.9763   time: 67.350    \n",
      "epoch: 0     step: 272000    loss: 0.0579    acc: 0.9783   accm: 0.9627   accmv: 0.9776   time: 63.215    \n",
      "epoch: 0     step: 274048    loss: 0.0576    acc: 0.9788   accm: 0.9637   accmv: 0.9779   time: 67.583    \n",
      "epoch: 0     step: 276096    loss: 0.0576    acc: 0.9786   accm: 0.9632   accmv: 0.9776   time: 67.511    \n",
      "epoch: 0     step: 278016    loss: 0.0579    acc: 0.9788   accm: 0.9636   accmv: 0.9763   time: 63.815    \n",
      "epoch: 0     step: 280064    loss: 0.0544    acc: 0.9800   accm: 0.9657   accmv: 0.9765   time: 67.852    \n",
      "epoch: 0     step: 282112    loss: 0.0556    acc: 0.9796   accm: 0.9649   accmv: 0.9758   time: 67.525    \n",
      "epoch: 0     step: 284032    loss: 0.0546    acc: 0.9799   accm: 0.9654   accmv: 0.9758   time: 62.638    \n",
      "epoch: 0     step: 286080    loss: 0.0542    acc: 0.9804   accm: 0.9664   accmv: 0.9783   time: 67.264    \n",
      "epoch: 0     step: 288000    loss: 0.0534    acc: 0.9805   accm: 0.9665   accmv: 0.9789   time: 63.394    \n",
      "epoch: 0     step: 290048    loss: 0.0556    acc: 0.9800   accm: 0.9657   accmv: 0.9776   time: 67.386    \n",
      "epoch: 0     step: 292096    loss: 0.0535    acc: 0.9800   accm: 0.9657   accmv: 0.9783   time: 67.543    \n",
      "epoch: 0     step: 294016    loss: 0.0523    acc: 0.9808   accm: 0.9670   accmv: 0.9779   time: 63.336    \n",
      "epoch: 0     step: 296064    loss: 0.0510    acc: 0.9807   accm: 0.9669   accmv: 0.9783   time: 67.796    \n",
      "epoch: 0     step: 298112    loss: 0.0531    acc: 0.9805   accm: 0.9665   accmv: 0.9756   time: 67.626    \n",
      "epoch: 0     step: 300032    loss: 0.0525    acc: 0.9808   accm: 0.9671   accmv: 0.9763   time: 63.430    \n",
      "epoch: 0     step: 302080    loss: 0.0518    acc: 0.9808   accm: 0.9670   accmv: 0.9769   time: 67.486    \n",
      "epoch: 0     step: 304000    loss: 0.0510    acc: 0.9813   accm: 0.9679   accmv: 0.9791   time: 62.674    \n",
      "epoch: 0     step: 306048    loss: 0.0506    acc: 0.9811   accm: 0.9675   accmv: 0.9788   time: 67.494    \n",
      "epoch: 0     step: 308096    loss: 0.0521    acc: 0.9806   accm: 0.9667   accmv: 0.9788   time: 67.493    \n",
      "epoch: 0     step: 310016    loss: 0.0506    acc: 0.9809   accm: 0.9672   accmv: 0.9781   time: 63.292    \n",
      "epoch: 0     step: 312064    loss: 0.0513    acc: 0.9810   accm: 0.9675   accmv: 0.9778   time: 67.404    \n",
      "epoch: 0     step: 314112    loss: 0.0499    acc: 0.9817   accm: 0.9685   accmv: 0.9781   time: 67.361    \n",
      "epoch: 0     step: 316032    loss: 0.0490    acc: 0.9819   accm: 0.9690   accmv: 0.9791   time: 63.699    \n",
      "epoch: 0     step: 318080    loss: 0.0495    acc: 0.9820   accm: 0.9690   accmv: 0.9814   time: 67.245    \n",
      "epoch: 0     step: 320000    loss: 0.0483    acc: 0.9822   accm: 0.9694   accmv: 0.9799   time: 63.454    \n",
      "epoch: 0     step: 322048    loss: 0.0475    acc: 0.9825   accm: 0.9699   accmv: 0.9789   time: 67.280    \n",
      "epoch: 0     step: 324096    loss: 0.0495    acc: 0.9814   accm: 0.9682   accmv: 0.9788   time: 67.486    \n",
      "epoch: 0     step: 326016    loss: 0.0470    acc: 0.9830   accm: 0.9709   accmv: 0.9783   time: 62.283    \n",
      "epoch: 0     step: 328064    loss: 0.0491    acc: 0.9817   accm: 0.9687   accmv: 0.9791   time: 67.321    \n",
      "epoch: 0     step: 330112    loss: 0.0480    acc: 0.9823   accm: 0.9696   accmv: 0.9793   time: 67.131    \n",
      "epoch: 0     step: 332032    loss: 0.0474    acc: 0.9824   accm: 0.9698   accmv: 0.9799   time: 63.410    \n",
      "epoch: 0     step: 334080    loss: 0.0477    acc: 0.9823   accm: 0.9696   accmv: 0.9783   time: 67.293    \n",
      "epoch: 0     step: 336000    loss: 0.0449    acc: 0.9834   accm: 0.9716   accmv: 0.9814   time: 63.228    \n",
      "epoch: 0     step: 338048    loss: 0.0444    acc: 0.9833   accm: 0.9712   accmv: 0.9803   time: 67.286    \n",
      "epoch: 0     step: 340096    loss: 0.0475    acc: 0.9827   accm: 0.9703   accmv: 0.9806   time: 67.231    \n",
      "epoch: 0     step: 342016    loss: 0.0463    acc: 0.9827   accm: 0.9703   accmv: 0.9803   time: 63.703    \n",
      "epoch: 0     step: 344064    loss: 0.0456    acc: 0.9834   accm: 0.9716   accmv: 0.9789   time: 67.336    \n",
      "epoch: 0     step: 346112    loss: 0.0441    acc: 0.9841   accm: 0.9727   accmv: 0.9806   time: 67.096    \n",
      "epoch: 0     step: 348032    loss: 0.0442    acc: 0.9840   accm: 0.9724   accmv: 0.9808   time: 62.676    \n",
      "epoch: 0     step: 350080    loss: 0.0450    acc: 0.9834   accm: 0.9715   accmv: 0.9784   time: 67.587    \n",
      "epoch: 0     step: 352000    loss: 0.0447    acc: 0.9833   accm: 0.9714   accmv: 0.9794   time: 63.459    \n",
      "epoch: 0     step: 354048    loss: 0.0452    acc: 0.9833   accm: 0.9714   accmv: 0.9798   time: 67.780    \n",
      "epoch: 0     step: 356096    loss: 0.0433    acc: 0.9842   accm: 0.9729   accmv: 0.9819   time: 67.996    \n",
      "epoch: 0     step: 358016    loss: 0.0432    acc: 0.9842   accm: 0.9728   accmv: 0.9809   time: 63.475    \n",
      "epoch: 0     step: 360064    loss: 0.0429    acc: 0.9839   accm: 0.9723   accmv: 0.9813   time: 67.438    \n",
      "epoch: 0     step: 362112    loss: 0.0427    acc: 0.9844   accm: 0.9731   accmv: 0.9834   time: 67.860    \n",
      "epoch: 0     step: 364032    loss: 0.0438    acc: 0.9837   accm: 0.9720   accmv: 0.9808   time: 63.713    \n",
      "epoch: 0     step: 366080    loss: 0.0429    acc: 0.9845   accm: 0.9733   accmv: 0.9811   time: 67.462    \n",
      "epoch: 0     step: 368000    loss: 0.0432    acc: 0.9839   accm: 0.9724   accmv: 0.9808   time: 62.465    \n",
      "epoch: 0     step: 370048    loss: 0.0420    acc: 0.9845   accm: 0.9734   accmv: 0.9808   time: 67.444    \n",
      "epoch: 0     step: 372096    loss: 0.0406    acc: 0.9849   accm: 0.9740   accmv: 0.9813   time: 67.858    \n",
      "epoch: 0     step: 374016    loss: 0.0417    acc: 0.9847   accm: 0.9738   accmv: 0.9809   time: 63.509    \n",
      "epoch: 0     step: 376064    loss: 0.0417    acc: 0.9846   accm: 0.9736   accmv: 0.9811   time: 67.343    \n",
      "epoch: 0     step: 378112    loss: 0.0416    acc: 0.9842   accm: 0.9729   accmv: 0.9831   time: 67.317    \n",
      "epoch: 0     step: 380032    loss: 0.0419    acc: 0.9846   accm: 0.9735   accmv: 0.9809   time: 63.734    \n",
      "epoch: 0     step: 382080    loss: 0.0410    acc: 0.9846   accm: 0.9735   accmv: 0.9791   time: 67.887    \n",
      "epoch: 0     step: 384000    loss: 0.0407    acc: 0.9852   accm: 0.9745   accmv: 0.9821   time: 63.967    \n",
      "epoch: 0     step: 386048    loss: 0.0398    acc: 0.9853   accm: 0.9748   accmv: 0.9811   time: 67.386    \n",
      "epoch: 0     step: 388096    loss: 0.0420    acc: 0.9843   accm: 0.9730   accmv: 0.9808   time: 67.766    \n",
      "epoch: 0     step: 390016    loss: 0.0410    acc: 0.9847   accm: 0.9737   accmv: 0.9821   time: 62.643    \n",
      "epoch: 0     step: 392064    loss: 0.0408    acc: 0.9847   accm: 0.9738   accmv: 0.9829   time: 67.517    \n",
      "epoch: 0     step: 394112    loss: 0.0406    acc: 0.9849   accm: 0.9741   accmv: 0.9818   time: 67.261    \n",
      "epoch: 0     step: 396032    loss: 0.0401    acc: 0.9852   accm: 0.9745   accmv: 0.9823   time: 63.541    \n",
      "epoch: 0     step: 398080    loss: 0.0392    acc: 0.9854   accm: 0.9749   accmv: 0.9814   time: 67.299    \n",
      "epoch: 0     step: 400000    loss: 0.0378    acc: 0.9859   accm: 0.9757   accmv: 0.9819   time: 63.550    \n",
      "epoch: 0     step: 402048    loss: 0.0403    acc: 0.9850   accm: 0.9742   accmv: 0.9828   time: 67.162    \n",
      "epoch: 0     step: 404096    loss: 0.0383    acc: 0.9858   accm: 0.9757   accmv: 0.9823   time: 67.083    \n",
      "epoch: 0     step: 406016    loss: 0.0398    acc: 0.9856   accm: 0.9753   accmv: 0.9829   time: 63.432    \n",
      "epoch: 0     step: 408064    loss: 0.0391    acc: 0.9855   accm: 0.9751   accmv: 0.9818   time: 67.193    \n",
      "epoch: 0     step: 410112    loss: 0.0397    acc: 0.9853   accm: 0.9748   accmv: 0.9842   time: 67.366    \n",
      "epoch: 0     step: 412032    loss: 0.0377    acc: 0.9859   accm: 0.9758   accmv: 0.9824   time: 62.433    \n",
      "epoch: 0     step: 414080    loss: 0.0379    acc: 0.9861   accm: 0.9762   accmv: 0.9842   time: 67.518    \n",
      "epoch: 0     step: 416000    loss: 0.0388    acc: 0.9857   accm: 0.9754   accmv: 0.9837   time: 63.652    \n",
      "epoch: 0     step: 418048    loss: 0.0415    acc: 0.9847   accm: 0.9737   accmv: 0.9841   time: 68.084    \n",
      "epoch: 0     step: 420096    loss: 0.0385    acc: 0.9860   accm: 0.9759   accmv: 0.9823   time: 68.332    \n",
      "epoch: 0     step: 422016    loss: 0.0391    acc: 0.9856   accm: 0.9753   accmv: 0.9828   time: 64.120    \n",
      "epoch: 0     step: 424064    loss: 0.0395    acc: 0.9853   accm: 0.9748   accmv: 0.9841   time: 68.117    \n",
      "epoch: 0     step: 426112    loss: 0.0385    acc: 0.9857   accm: 0.9754   accmv: 0.9837   time: 68.049    \n",
      "epoch: 0     step: 428032    loss: 0.0366    acc: 0.9865   accm: 0.9769   accmv: 0.9841   time: 63.863    \n",
      "epoch: 0     step: 430080    loss: 0.0378    acc: 0.9861   accm: 0.9761   accmv: 0.9833   time: 67.663    \n",
      "epoch: 0     step: 432000    loss: 0.0379    acc: 0.9858   accm: 0.9757   accmv: 0.9841   time: 62.983    \n",
      "epoch: 0     step: 434048    loss: 0.0378    acc: 0.9861   accm: 0.9761   accmv: 0.9837   time: 68.073    \n",
      "epoch: 0     step: 436096    loss: 0.0358    acc: 0.9865   accm: 0.9768   accmv: 0.9849   time: 67.744    \n",
      "epoch: 0     step: 438016    loss: 0.0377    acc: 0.9861   accm: 0.9762   accmv: 0.9846   time: 64.232    \n",
      "epoch: 0     step: 440064    loss: 0.0362    acc: 0.9867   accm: 0.9772   accmv: 0.9841   time: 67.906    \n",
      "epoch: 0     step: 442112    loss: 0.0357    acc: 0.9867   accm: 0.9772   accmv: 0.9851   time: 67.825    \n",
      "epoch: 0     step: 444032    loss: 0.0358    acc: 0.9867   accm: 0.9771   accmv: 0.9834   time: 63.895    \n",
      "epoch: 0     step: 446080    loss: 0.0370    acc: 0.9863   accm: 0.9764   accmv: 0.9846   time: 67.819    \n",
      "epoch: 0     step: 448000    loss: 0.0355    acc: 0.9870   accm: 0.9776   accmv: 0.9839   time: 64.104    \n",
      "epoch: 0     step: 450048    loss: 0.0354    acc: 0.9872   accm: 0.9781   accmv: 0.9841   time: 68.296    \n",
      "epoch: 0     step: 452096    loss: 0.0359    acc: 0.9864   accm: 0.9766   accmv: 0.9842   time: 67.694    \n",
      "epoch: 0     step: 454016    loss: 0.0355    acc: 0.9871   accm: 0.9778   accmv: 0.9841   time: 63.331    \n",
      "epoch: 0     step: 456064    loss: 0.0352    acc: 0.9870   accm: 0.9776   accmv: 0.9842   time: 68.001    \n",
      "epoch: 0     step: 458112    loss: 0.0363    acc: 0.9864   accm: 0.9766   accmv: 0.9859   time: 67.952    \n",
      "epoch: 0     step: 460032    loss: 0.0347    acc: 0.9871   accm: 0.9778   accmv: 0.9856   time: 64.062    \n",
      "epoch: 0     step: 462080    loss: 0.0359    acc: 0.9868   accm: 0.9773   accmv: 0.9857   time: 68.180    \n",
      "epoch: 0     step: 464000    loss: 0.0348    acc: 0.9871   accm: 0.9779   accmv: 0.9841   time: 64.260    \n",
      "epoch: 0     step: 466048    loss: 0.0339    acc: 0.9877   accm: 0.9789   accmv: 0.9859   time: 67.683    \n",
      "epoch: 0     step: 468096    loss: 0.0357    acc: 0.9867   accm: 0.9772   accmv: 0.9854   time: 68.150    \n",
      "epoch: 0     step: 470016    loss: 0.0356    acc: 0.9870   accm: 0.9777   accmv: 0.9846   time: 63.773    \n",
      "epoch: 0     step: 472064    loss: 0.0351    acc: 0.9871   accm: 0.9779   accmv: 0.9861   time: 68.075    \n",
      "epoch: 0     step: 474112    loss: 0.0350    acc: 0.9867   accm: 0.9772   accmv: 0.9854   time: 67.888    \n",
      "epoch: 0     step: 476032    loss: 0.0338    acc: 0.9876   accm: 0.9786   accmv: 0.9844   time: 63.437    \n",
      "epoch: 0     step: 478080    loss: 0.0334    acc: 0.9873   accm: 0.9782   accmv: 0.9849   time: 68.358    \n",
      "epoch: 0     step: 480000    loss: 0.0346    acc: 0.9872   accm: 0.9780   accmv: 0.9857   time: 63.852    \n",
      "epoch: 0     step: 482048    loss: 0.0326    acc: 0.9881   accm: 0.9795   accmv: 0.9852   time: 67.753    \n",
      "epoch: 0     step: 484096    loss: 0.0344    acc: 0.9873   accm: 0.9782   accmv: 0.9847   time: 67.772    \n",
      "epoch: 0     step: 486016    loss: 0.0329    acc: 0.9880   accm: 0.9794   accmv: 0.9841   time: 63.494    \n",
      "epoch: 0     step: 488064    loss: 0.0330    acc: 0.9878   accm: 0.9791   accmv: 0.9856   time: 67.882    \n",
      "epoch: 0     step: 490112    loss: 0.0336    acc: 0.9874   accm: 0.9783   accmv: 0.9819   time: 67.894    \n",
      "epoch: 0     step: 492032    loss: 0.0348    acc: 0.9869   accm: 0.9775   accmv: 0.9833   time: 64.093    \n",
      "epoch: 0     step: 494080    loss: 0.0331    acc: 0.9880   accm: 0.9794   accmv: 0.9859   time: 67.985    \n",
      "epoch: 0     step: 496000    loss: 0.0341    acc: 0.9873   accm: 0.9781   accmv: 0.9856   time: 62.935    \n",
      "epoch: 0     step: 498048    loss: 0.0325    acc: 0.9880   accm: 0.9793   accmv: 0.9842   time: 67.659    \n",
      "epoch: 0     step: 500096    loss: 0.0338    acc: 0.9874   accm: 0.9784   accmv: 0.9837   time: 67.492    \n",
      "epoch: 0     step: 502016    loss: 0.0323    acc: 0.9879   accm: 0.9793   accmv: 0.9852   time: 63.618    \n",
      "epoch: 0     step: 504064    loss: 0.0324    acc: 0.9881   accm: 0.9796   accmv: 0.9851   time: 67.360    \n",
      "epoch: 0     step: 506112    loss: 0.0327    acc: 0.9877   accm: 0.9788   accmv: 0.9859   time: 67.614    \n",
      "epoch: 0     step: 508032    loss: 0.0324    acc: 0.9878   accm: 0.9790   accmv: 0.9846   time: 63.712    \n",
      "epoch: 0     step: 510080    loss: 0.0326    acc: 0.9880   accm: 0.9794   accmv: 0.9847   time: 67.439    \n",
      "epoch: 0     step: 512000    loss: 0.0312    acc: 0.9885   accm: 0.9802   accmv: 0.9856   time: 64.017    \n",
      "epoch: 0     step: 514048    loss: 0.0319    acc: 0.9884   accm: 0.9800   accmv: 0.9852   time: 67.894    \n",
      "epoch: 0     step: 516096    loss: 0.0305    acc: 0.9888   accm: 0.9807   accmv: 0.9851   time: 67.822    \n",
      "epoch: 0     step: 518016    loss: 0.0332    acc: 0.9878   accm: 0.9791   accmv: 0.9852   time: 62.621    \n",
      "epoch: 0     step: 520064    loss: 0.0336    acc: 0.9874   accm: 0.9784   accmv: 0.9872   time: 67.832    \n",
      "epoch: 0     step: 522112    loss: 0.0316    acc: 0.9882   accm: 0.9798   accmv: 0.9854   time: 67.780    \n",
      "epoch: 0     step: 524032    loss: 0.0342    acc: 0.9874   accm: 0.9784   accmv: 0.9844   time: 63.435    \n",
      "epoch: 0     step: 526080    loss: 0.0318    acc: 0.9884   accm: 0.9801   accmv: 0.9859   time: 67.803    \n",
      "epoch: 0     step: 528000    loss: 0.0345    acc: 0.9869   accm: 0.9776   accmv: 0.9867   time: 64.060    \n",
      "epoch: 0     step: 530048    loss: 0.0350    acc: 0.9871   accm: 0.9779   accmv: 0.9859   time: 67.783    \n",
      "epoch: 0     step: 532096    loss: 0.0310    acc: 0.9883   accm: 0.9799   accmv: 0.9851   time: 68.098    \n",
      "epoch: 0     step: 534016    loss: 0.0303    acc: 0.9887   accm: 0.9806   accmv: 0.9839   time: 63.630    \n",
      "epoch: 0     step: 536064    loss: 0.0322    acc: 0.9880   accm: 0.9793   accmv: 0.9859   time: 67.804    \n",
      "epoch: 0     step: 538112    loss: 0.0321    acc: 0.9884   accm: 0.9801   accmv: 0.9852   time: 67.460    \n",
      "epoch: 0     step: 540032    loss: 0.0310    acc: 0.9884   accm: 0.9800   accmv: 0.9859   time: 62.825    \n",
      "epoch: 0     step: 542080    loss: 0.0304    acc: 0.9886   accm: 0.9805   accmv: 0.9861   time: 67.744    \n",
      "epoch: 0     step: 544000    loss: 0.0307    acc: 0.9885   accm: 0.9803   accmv: 0.9859   time: 63.921    \n",
      "epoch: 0     step: 546048    loss: 0.0320    acc: 0.9879   accm: 0.9793   accmv: 0.9847   time: 67.511    \n",
      "epoch: 0     step: 548096    loss: 0.0316    acc: 0.9879   accm: 0.9792   accmv: 0.9851   time: 67.903    \n",
      "epoch: 0     step: 550016    loss: 0.0300    acc: 0.9888   accm: 0.9807   accmv: 0.9874   time: 63.795    \n",
      "epoch: 0     step: 552064    loss: 0.0316    acc: 0.9883   accm: 0.9798   accmv: 0.9852   time: 67.649    \n",
      "epoch: 0     step: 554112    loss: 0.0298    acc: 0.9891   accm: 0.9812   accmv: 0.9854   time: 67.998    \n",
      "epoch: 0     step: 556032    loss: 0.0296    acc: 0.9891   accm: 0.9813   accmv: 0.9857   time: 63.765    \n",
      "epoch: 0     step: 558080    loss: 0.0295    acc: 0.9890   accm: 0.9812   accmv: 0.9874   time: 67.855    \n",
      "epoch: 0     step: 560000    loss: 0.0301    acc: 0.9888   accm: 0.9807   accmv: 0.9859   time: 62.887    \n",
      "epoch: 0     step: 562048    loss: 0.0316    acc: 0.9883   accm: 0.9800   accmv: 0.9864   time: 67.693    \n",
      "epoch: 0     step: 564096    loss: 0.0290    acc: 0.9894   accm: 0.9818   accmv: 0.9864   time: 67.770    \n",
      "epoch: 0     step: 566016    loss: 0.0308    acc: 0.9883   accm: 0.9800   accmv: 0.9857   time: 63.759    \n",
      "epoch: 0     step: 568064    loss: 0.0301    acc: 0.9889   accm: 0.9809   accmv: 0.9867   time: 67.919    \n",
      "epoch: 0     step: 570112    loss: 0.0291    acc: 0.9895   accm: 0.9819   accmv: 0.9851   time: 68.428    \n",
      "epoch: 0     step: 572032    loss: 0.0293    acc: 0.9894   accm: 0.9819   accmv: 0.9859   time: 64.350    \n",
      "epoch: 0     step: 574080    loss: 0.0296    acc: 0.9890   accm: 0.9812   accmv: 0.9876   time: 68.243    \n",
      "epoch: 0     step: 576000    loss: 0.0304    acc: 0.9886   accm: 0.9805   accmv: 0.9874   time: 64.303    \n",
      "epoch: 0     step: 578048    loss: 0.0303    acc: 0.9886   accm: 0.9805   accmv: 0.9861   time: 67.806    \n",
      "epoch: 0     step: 580096    loss: 0.0295    acc: 0.9893   accm: 0.9816   accmv: 0.9859   time: 67.819    \n",
      "epoch: 0     step: 582016    loss: 0.0303    acc: 0.9885   accm: 0.9803   accmv: 0.9874   time: 63.170    \n",
      "epoch: 0     step: 584064    loss: 0.0276    acc: 0.9898   accm: 0.9825   accmv: 0.9844   time: 68.201    \n",
      "epoch: 0     step: 586112    loss: 0.0307    acc: 0.9885   accm: 0.9803   accmv: 0.9866   time: 68.046    \n",
      "epoch: 0     step: 588032    loss: 0.0293    acc: 0.9892   accm: 0.9815   accmv: 0.9861   time: 63.800    \n",
      "epoch: 0     step: 590080    loss: 0.0293    acc: 0.9890   accm: 0.9811   accmv: 0.9854   time: 67.404    \n",
      "epoch: 0     step: 592000    loss: 0.0300    acc: 0.9889   accm: 0.9810   accmv: 0.9866   time: 63.532    \n",
      "epoch: 0     step: 594048    loss: 0.0294    acc: 0.9892   accm: 0.9814   accmv: 0.9867   time: 68.172    \n",
      "epoch: 0     step: 596096    loss: 0.0301    acc: 0.9891   accm: 0.9814   accmv: 0.9861   time: 67.857    \n",
      "epoch: 0     step: 598016    loss: 0.0287    acc: 0.9892   accm: 0.9815   accmv: 0.9859   time: 63.907    \n",
      "epoch: 0     step: 600064    loss: 0.0302    acc: 0.9889   accm: 0.9809   accmv: 0.9847   time: 67.873    \n",
      "epoch: 0     step: 602112    loss: 0.0284    acc: 0.9895   accm: 0.9820   accmv: 0.9859   time: 67.704    \n",
      "epoch: 0     step: 604032    loss: 0.0280    acc: 0.9895   accm: 0.9821   accmv: 0.9867   time: 62.913    \n",
      "epoch: 0     step: 606080    loss: 0.0275    acc: 0.9897   accm: 0.9824   accmv: 0.9862   time: 67.581    \n",
      "epoch: 0     step: 608000    loss: 0.0284    acc: 0.9894   accm: 0.9818   accmv: 0.9871   time: 63.651    \n",
      "epoch: 0     step: 610048    loss: 0.0296    acc: 0.9890   accm: 0.9811   accmv: 0.9851   time: 67.534    \n",
      "epoch: 0     step: 612096    loss: 0.0295    acc: 0.9890   accm: 0.9811   accmv: 0.9864   time: 67.869    \n",
      "epoch: 0     step: 614016    loss: 0.0280    acc: 0.9895   accm: 0.9819   accmv: 0.9866   time: 64.050    \n",
      "epoch: 0     step: 616064    loss: 0.0281    acc: 0.9895   accm: 0.9819   accmv: 0.9859   time: 67.802    \n",
      "epoch: 0     step: 618112    loss: 0.0274    acc: 0.9898   accm: 0.9825   accmv: 0.9861   time: 67.464    \n",
      "epoch: 0     step: 620032    loss: 0.0274    acc: 0.9899   accm: 0.9826   accmv: 0.9864   time: 63.611    \n",
      "epoch: 0     step: 622080    loss: 0.0278    acc: 0.9895   accm: 0.9820   accmv: 0.9867   time: 67.882    \n",
      "epoch: 0     step: 624000    loss: 0.0276    acc: 0.9897   accm: 0.9823   accmv: 0.9857   time: 62.773    \n",
      "epoch: 0     step: 626048    loss: 0.0297    acc: 0.9890   accm: 0.9811   accmv: 0.9861   time: 67.865    \n",
      "epoch: 0     step: 628096    loss: 0.0272    acc: 0.9901   accm: 0.9829   accmv: 0.9871   time: 67.713    \n",
      "epoch: 0     step: 630016    loss: 0.0284    acc: 0.9894   accm: 0.9818   accmv: 0.9856   time: 63.645    \n",
      "epoch: 0     step: 632064    loss: 0.0268    acc: 0.9900   accm: 0.9828   accmv: 0.9852   time: 67.874    \n",
      "epoch: 0     step: 634112    loss: 0.0255    acc: 0.9904   accm: 0.9836   accmv: 0.9872   time: 67.771    \n",
      "epoch: 0     step: 636032    loss: 0.0268    acc: 0.9900   accm: 0.9829   accmv: 0.9871   time: 63.802    \n",
      "epoch: 0     step: 638080    loss: 0.0278    acc: 0.9895   accm: 0.9820   accmv: 0.9857   time: 67.915    \n",
      "epoch: 0     step: 640000    loss: 0.0275    acc: 0.9895   accm: 0.9820   accmv: 0.9871   time: 63.670    \n",
      "epoch: 0     step: 642048    loss: 0.0270    acc: 0.9899   accm: 0.9827   accmv: 0.9874   time: 67.665    \n",
      "epoch: 0     step: 644096    loss: 0.0272    acc: 0.9899   accm: 0.9826   accmv: 0.9862   time: 67.509    \n",
      "epoch: 0     step: 646016    loss: 0.0286    acc: 0.9895   accm: 0.9819   accmv: 0.9869   time: 62.941    \n",
      "epoch: 0     step: 648064    loss: 0.0273    acc: 0.9897   accm: 0.9824   accmv: 0.9861   time: 67.779    \n",
      "epoch: 0     step: 650112    loss: 0.0279    acc: 0.9896   accm: 0.9821   accmv: 0.9887   time: 68.123    \n",
      "epoch: 0     step: 652032    loss: 0.0259    acc: 0.9905   accm: 0.9836   accmv: 0.9879   time: 63.684    \n",
      "epoch: 0     step: 654080    loss: 0.0255    acc: 0.9905   accm: 0.9836   accmv: 0.9859   time: 67.529    \n",
      "epoch: 0     step: 656000    loss: 0.0251    acc: 0.9907   accm: 0.9841   accmv: 0.9876   time: 63.628    \n",
      "epoch: 0     step: 658048    loss: 0.0271    acc: 0.9902   accm: 0.9832   accmv: 0.9874   time: 68.192    \n",
      "epoch: 0     step: 660096    loss: 0.0271    acc: 0.9899   accm: 0.9826   accmv: 0.9864   time: 67.626    \n",
      "epoch: 0     step: 662016    loss: 0.0262    acc: 0.9903   accm: 0.9834   accmv: 0.9864   time: 64.030    \n",
      "epoch: 0     step: 664064    loss: 0.0266    acc: 0.9898   accm: 0.9825   accmv: 0.9876   time: 67.733    \n",
      "epoch: 0     step: 666112    loss: 0.0270    acc: 0.9900   accm: 0.9829   accmv: 0.9857   time: 68.130    \n",
      "epoch: 0     step: 668032    loss: 0.0261    acc: 0.9902   accm: 0.9832   accmv: 0.9859   time: 63.044    \n",
      "epoch: 0     step: 670080    loss: 0.0275    acc: 0.9898   accm: 0.9825   accmv: 0.9864   time: 68.104    \n",
      "epoch: 0     step: 672000    loss: 0.0254    acc: 0.9906   accm: 0.9839   accmv: 0.9881   time: 64.239    \n",
      "epoch: 0     step: 674048    loss: 0.0274    acc: 0.9899   accm: 0.9826   accmv: 0.9866   time: 67.564    \n",
      "epoch: 0     step: 676096    loss: 0.0256    acc: 0.9904   accm: 0.9835   accmv: 0.9866   time: 67.808    \n",
      "epoch: 0     step: 678016    loss: 0.0265    acc: 0.9902   accm: 0.9832   accmv: 0.9866   time: 63.859    \n",
      "epoch: 0     step: 680064    loss: 0.0275    acc: 0.9896   accm: 0.9822   accmv: 0.9864   time: 67.574    \n",
      "epoch: 0     step: 682112    loss: 0.0270    acc: 0.9902   accm: 0.9831   accmv: 0.9851   time: 67.922    \n",
      "epoch: 0     step: 684032    loss: 0.0261    acc: 0.9904   accm: 0.9834   accmv: 0.9864   time: 63.803    \n",
      "epoch: 0     step: 686080    loss: 0.0259    acc: 0.9903   accm: 0.9834   accmv: 0.9872   time: 67.714    \n",
      "epoch: 0     step: 688000    loss: 0.0257    acc: 0.9904   accm: 0.9835   accmv: 0.9867   time: 62.784    \n",
      "epoch: 0     step: 690048    loss: 0.0260    acc: 0.9904   accm: 0.9834   accmv: 0.9876   time: 67.663    \n",
      "epoch: 0     step: 692096    loss: 0.0265    acc: 0.9902   accm: 0.9832   accmv: 0.9864   time: 67.420    \n",
      "epoch: 0     step: 694016    loss: 0.0273    acc: 0.9898   accm: 0.9825   accmv: 0.9864   time: 63.566    \n",
      "epoch: 0     step: 696064    loss: 0.0254    acc: 0.9905   accm: 0.9837   accmv: 0.9872   time: 67.464    \n",
      "epoch: 0     step: 698112    loss: 0.0242    acc: 0.9911   accm: 0.9847   accmv: 0.9862   time: 67.478    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-cae0288c38eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mstate_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"2019-03-26-sudoku-4xEdgeConv-{epoch}-tab4-09852.state\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# Training dataset\n",
    "def collate_fn(x):\n",
    "    return x\n",
    "\n",
    "dataloader = DataLoader(sudoku_dataset_train, shuffle=True, num_workers=4, batch_size=128, collate_fn=collate_fn)\n",
    "x_onehot_lst = [torch.FloatTensor(81, 10).to(device) for _ in range(128)]\n",
    "\n",
    "# Validation dataset (subset)\n",
    "validation_batch = [sudoku_dataset_valid[i].to(device) for i in range(128)]\n",
    "validation_data_lst = []\n",
    "for data in validation_batch:\n",
    "    x_onehot = torch.FloatTensor(81, 10).to(device)\n",
    "    x_onehot.zero_()\n",
    "    x_onehot.scatter_(1, data.x, 1)\n",
    "    validation_data_lst.append((data.x.squeeze(1).detach(), data.edge_index.detach()))\n",
    "\n",
    "info_size = 2_000\n",
    "for epoch in range(0, 5):\n",
    "    running_loss = 0\n",
    "    running_loss_count = 0\n",
    "    running_total = 0\n",
    "    running_correct = 0\n",
    "    running_total_missing = 0\n",
    "    running_correct_missing = 0\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data_lst = []\n",
    "        for j, data in enumerate(batch):\n",
    "            data = data.to(device)\n",
    "            x_onehot = x_onehot_lst[j]\n",
    "            x_onehot.zero_()\n",
    "            x_onehot.scatter_(1, data.x, 1)\n",
    "            data_lst.append((data.x.squeeze(1), data.edge_index))\n",
    "\n",
    "        # Train\n",
    "        net.train()\n",
    "        outputs_lst = net(data_lst)\n",
    "\n",
    "        outputs_all = torch.cat(outputs_lst)\n",
    "        labels_all = torch.cat([(data.y - 1).squeeze() for data in batch])\n",
    "        loss = criterion(outputs_all, labels_all)\n",
    "        loss.backward()\n",
    "\n",
    "        running_loss += loss.detach().item()\n",
    "        running_loss_count += 1\n",
    "\n",
    "        # Accuracy for all\n",
    "        _, predicted = torch.max(outputs_all.data, 1)\n",
    "        running_correct += (predicted == labels_all).sum().item()\n",
    "        running_total += len(predicted)\n",
    "\n",
    "        # Accuracy for missing only\n",
    "        outputs_missing_lst = []\n",
    "        labels_missing_lst = []\n",
    "        for (data, outputs) in zip(batch, outputs_lst):\n",
    "            mask = (data.x == 0).squeeze()\n",
    "            outputs_missing_lst.append(outputs[mask])\n",
    "            labels = (data.y - 1).squeeze()\n",
    "            labels_missing_lst.append(labels[mask])\n",
    "        outputs_missing = torch.cat(outputs_missing_lst)\n",
    "        labels_missing = torch.cat(labels_missing_lst)\n",
    "\n",
    "        _, predicted = torch.max(outputs_missing.data, 1)\n",
    "        running_correct_missing += (predicted == labels_missing).sum().item()\n",
    "        running_total_missing += len(predicted)\n",
    "\n",
    "        if i * 128 % info_size < 128:\n",
    "            # Eval\n",
    "            net.eval()\n",
    "            outputs_lst_ref = net(validation_data_lst)\n",
    "\n",
    "            outputs_missing_lst = []\n",
    "            labels_missing_lst = []\n",
    "            for (data, outputs) in zip(validation_batch, outputs_lst_ref):\n",
    "                mask = (data.x == 0).squeeze()\n",
    "                outputs_missing_lst.append(outputs[mask])\n",
    "                labels = (data.y - 1).squeeze()\n",
    "                labels_missing_lst.append(labels[mask])\n",
    "\n",
    "            outputs_missing = torch.cat(outputs_missing_lst)\n",
    "            labels_missing = torch.cat(labels_missing_lst)\n",
    "\n",
    "            _, predicted = torch.max(outputs_missing.data, 1)\n",
    "            correct = (predicted == labels_missing).sum().item()\n",
    "            total = len(predicted)\n",
    "\n",
    "            print(\"{:13s}{:16s}{:16s}{:14s}{:15s}{:16s}{:16s}\".format(\n",
    "                f\"epoch: {epoch}\",\n",
    "                f\"step: {i * 128}\",\n",
    "                f\"loss: {running_loss / running_loss_count:.4f}\",\n",
    "                f\"acc: {running_correct / running_total:.4f}\",\n",
    "                f\"accm: {running_correct_missing / running_total_missing:.4f}\",\n",
    "                f\"accmv: {correct / total:.4f}\",\n",
    "                f\"time: {time.perf_counter() - start_time:.3f}\",\n",
    "            ))\n",
    "            running_loss = 0\n",
    "            running_loss_count = 0\n",
    "            running_total = 0\n",
    "            running_correct = 0\n",
    "            running_total_missing = 0\n",
    "            running_correct_missing = 0\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "    state_file = f\"2019-03-26-sudoku-4xEdgeConv-{epoch}-tab4-09852.state\"\n",
    "    torch.save(net.state_dict(), state_file)\n",
    "    files.download(state_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1_M0Ep72DZt"
   },
   "outputs": [],
   "source": [
    "files.download(state_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1fXJz1l9HvhW",
    "u9q4iTifHvhl"
   ],
   "name": "Copy of 2019-03-30-sudoku-4xEdgeConv-09862+.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
