{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_gnhLD0HvhU",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fXJz1l9HvhW"
   },
   "source": [
    "### Install `pytorch_geometric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "hIbvKDk_HvhX",
    "outputId": "349cb6d3-ffbc-4550-d9dd-dd6288ed5356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-scatter\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/83/67eeea00c2db1959e2ff95d8680dbd756977bfab254bda8658f09dc3bc11/torch_scatter-1.1.2.tar.gz\n",
      "Building wheels for collected packages: torch-scatter\n",
      "  Building wheel for torch-scatter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a8/41/ef/1a52be728eedba23aa6d39b0bd6e9b533cb7ff572e967a6a43\n",
      "Successfully built torch-scatter\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "tw-y9MmIHvha",
    "outputId": "0666864a-5017-40a1-a77c-d23388eb0b1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-sparse\n",
      "  Downloading https://files.pythonhosted.org/packages/73/72/e374662f6f47d9ac0e082a6d5c18d14e15c52863e89c6bc6957a0d2ed026/torch_sparse-0.2.4.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse) (1.14.6)\n",
      "Building wheels for collected packages: torch-sparse\n",
      "  Building wheel for torch-sparse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/14/8f/77/7be0a2e42c5d1a6257b71edf5dc42f9e148d2137b66d869ee0\n",
      "Successfully built torch-sparse\n",
      "Installing collected packages: torch-sparse\n",
      "Successfully installed torch-sparse-0.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "n3SMtaYHHvhc",
    "outputId": "6250a80b-b32d-4fc8-cf90-ad0507e83ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-cluster\n",
      "  Downloading https://files.pythonhosted.org/packages/32/c8/9b3af10be647326dd807bb2fe7ced8ae4c3fd74178dba884621749afc4d7/torch_cluster-1.2.4.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from torch-cluster) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-cluster) (1.14.6)\n",
      "Building wheels for collected packages: torch-cluster\n",
      "  Building wheel for torch-cluster (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/5b/54/4a/ae0125851936c2b5ea13dddb1ab4b103b9f71e0f3211a6c4e1\n",
      "Successfully built torch-cluster\n",
      "Installing collected packages: torch-cluster\n",
      "Successfully installed torch-cluster-1.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "4AQ_A4XLHvhg",
    "outputId": "601b9d38-8291-431d-a90b-046bb233469a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-spline-conv\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/6d/b34721af4bb907814a41a1e0e5e426c97aee644efef6877ed112bfb3d81c/torch_spline_conv-1.0.6.tar.gz\n",
      "Building wheels for collected packages: torch-spline-conv\n",
      "  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f6/e2/d7/d87645a1e1d34b51633b7fd13f020b618c0ee84000b4f4085c\n",
      "Successfully built torch-spline-conv\n",
      "Installing collected packages: torch-spline-conv\n",
      "Successfully installed torch-spline-conv-1.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-spline-conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "colab_type": "code",
    "id": "uk6E9JP_Hvhj",
    "outputId": "5a986a43-4d09-41ab-efb6-9fb1cab56cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/4d/06dd0d277bf82ff4e1888f73b4c522c35f017505acacc25ecc95befaac6e/torch_geometric-1.1.0.tar.gz (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 2.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.14.6)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.1.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.20.3)\n",
      "Collecting plyfile (from torch-geometric)\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/82/d4069cbb49954d44087c37ff616cb423d3e2c0dd276378cdb4af3e3ef2ee/plyfile-0.7.tar.gz\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.0)\n",
      "Collecting rdflib (from torch-geometric)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
      "\u001b[K    100% |████████████████████████████████| 348kB 24.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.5.3)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.3.1)\n",
      "Collecting isodate (from rdflib->torch-geometric)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 14.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas->torch-geometric) (1.11.0)\n",
      "Building wheels for collected packages: torch-geometric, plyfile\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3e/c1/87/daa10b8fa568fb9a84ff1d9c24582af1ecc7e797e5e69ff58e\n",
      "  Building wheel for plyfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/91/3e/ee/e5630ef0fd53cedaa6e911ba27e8b40fff034388d1f264bb92\n",
      "Successfully built torch-geometric plyfile\n",
      "Installing collected packages: plyfile, isodate, rdflib, torch-geometric\n",
      "Successfully installed isodate-0.6.0 plyfile-0.7 rdflib-4.2.2 torch-geometric-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9q4iTifHvhl"
   },
   "source": [
    "### Install `proteinsolver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "colab_type": "code",
    "id": "HCWw1IgNHvhm",
    "outputId": "fcb1a352-1ca8-4708-c3af-11bb0f9a3c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://gitlab%2Bdeploy-token-52110:****@gitlab.com/ostrokach/proteinsolver.git\n",
      "  Cloning https://gitlab%2Bdeploy-token-52110:****@gitlab.com/ostrokach/proteinsolver.git to /tmp/pip-req-build-u2bdbexg\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: proteinsolver\n",
      "  Building wheel for proteinsolver (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-6rtg4lok/wheels/fb/05/26/209b40c24fc617333b311553f1c37610fb9b3475d67811bfcc\n",
      "Successfully built proteinsolver\n",
      "Installing collected packages: proteinsolver\n",
      "Successfully installed proteinsolver-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://gitlab.com/ostrokach/proteinsolver.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eoBMUoW2Hvhp",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "EmCKag-5MCiB",
    "outputId": "40059204-55bf-4285-eb0a-34098e980c40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 11 09:50:59 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN V             On   | 00000000:05:00.0  On |                  N/A |\n",
      "| 61%   84C    P2   144W / 250W |  11437MiB / 12033MiB |    100%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN Xp            On   | 00000000:06:00.0 Off |                  N/A |\n",
      "| 55%   84C    P2   179W / 250W |   8622MiB / 12196MiB |    100%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN Xp            On   | 00000000:09:00.0 Off |                  N/A |\n",
      "| 23%   28C    P8     9W / 250W |     12MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     12647      G   /usr/lib/xorg/Xorg                            39MiB |\n",
      "|    0     12684      G   /usr/bin/gnome-shell                          78MiB |\n",
      "|    0     26217      C   ...apkg-adjacency-net-v2-test81/bin/python   915MiB |\n",
      "|    0     29638      C   ...apkg-adjacency-net-v2-test81/bin/python 10391MiB |\n",
      "|    1     20366      C   ...apkg-adjacency-net-v2-test81/bin/python  8009MiB |\n",
      "|    1     26217      C   ...apkg-adjacency-net-v2-test81/bin/python   601MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbKxMUZWHvhq"
   },
   "outputs": [],
   "source": [
    "import atexit\n",
    "import csv\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from collections import deque\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import ChebConv, EdgeConv, GATConv, GCNConv\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, scatter_\n",
    "\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "rczy7pPiHvhs",
    "outputId": "8e4673d7-4cc7-4f8e-ffc4-de94302e5fab"
   },
   "outputs": [],
   "source": [
    "import proteinsolver\n",
    "import proteinsolver.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fFnAyUOHvhv"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsEY3dtLHvhy",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/kimlab1/strokach/working/proteinsolver/notebooks/sudoku_4xEdgeConv_uniform_192')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOTEBOOK_NAME = \"sudoku_4xEdgeConv_uniform_192\"\n",
    "NOTEBOOK_PATH = Path(NOTEBOOK_NAME).resolve()\n",
    "NOTEBOOK_PATH.mkdir(exist_ok=True)\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/strokach/ml_data')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT = Path(tempfile.gettempdir())\n",
    "DATA_ROOT = Path(\"/home/strokach/ml_data\")\n",
    "DATA_ROOT.mkdir(exist_ok=True)\n",
    "DATA_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sudoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZP3bLDO9PgT9"
   },
   "outputs": [],
   "source": [
    "sudoku_dataset_train = proteinsolver.datasets.SudokuDataset2(root=DATA_ROOT.joinpath(\"sudoku_train\"), subset=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudoku_dataset_valid = proteinsolver.datasets.SudokuDataset2(root=DATA_ROOT.joinpath(\"sudoku_valid\"), subset=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudoku_dataset_test = proteinsolver.datasets.SudokuDataset2(root=DATA_ROOT.joinpath(\"sudoku_test\"), subset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3kWuRaxr89h"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xGLXgqYlr89j"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import ChebConv, EdgeConv, GATConv, GCNConv\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, scatter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72WWaQOXJDdr"
   },
   "outputs": [],
   "source": [
    "class EdgeConvMod(torch.nn.Module):\n",
    "    def __init__(self, nn, aggr=\"max\"):\n",
    "        super().__init__()\n",
    "        self.nn = nn\n",
    "        self.aggr = aggr\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        \"\"\"\"\"\"\n",
    "        row, col = edge_index\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "\n",
    "        # TODO: Try -x[col] instead of x[col] - x[row]\n",
    "        if edge_attr is None:\n",
    "            out = torch.cat([x[row], x[col] - x[row]], dim=-1)\n",
    "        else:\n",
    "            out = torch.cat([x[row], x[col] - x[row], edge_attr], dim=-1)\n",
    "        out = self.nn(out)\n",
    "        x = scatter_(self.aggr, out, row, dim_size=x.size(0))\n",
    "\n",
    "        return x, out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}(nn={})\".format(self.__class__.__name__, self.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDrlxheizsKg"
   },
   "outputs": [],
   "source": [
    "class EdgeConvBatch(nn.Module):\n",
    "    def __init__(self, gnn, hidden_size, batch_norm=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gnn = gnn\n",
    "\n",
    "        x_post_modules = []\n",
    "        edge_attr_post_modules = []\n",
    "\n",
    "        if batch_norm is not None:\n",
    "            x_post_modules.append(nn.BatchNorm1d(hidden_size))\n",
    "            edge_attr_post_modules.append(nn.BatchNorm1d(hidden_size))\n",
    "\n",
    "        if dropout:\n",
    "            x_post_modules.append(nn.Dropout(dropout))\n",
    "            edge_attr_post_modules.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.x_postprocess = nn.Sequential(*x_post_modules)\n",
    "        self.edge_attr_postprocess = nn.Sequential(*edge_attr_post_modules)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        x, edge_attr = self.gnn(x, edge_index, edge_attr)\n",
    "        x = self.x_postprocess(x)\n",
    "        edge_attr = self.edge_attr_postprocess(edge_attr)\n",
    "        return x, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNCaSlrFDGA6"
   },
   "outputs": [],
   "source": [
    "def get_graph_conv_layer(input_size, hidden_size, output_size):\n",
    "    mlp = nn.Sequential(\n",
    "        #\n",
    "        nn.Linear(input_size, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, output_size),\n",
    "    )\n",
    "    gnn = EdgeConvMod(nn=mlp, aggr=\"add\")\n",
    "    graph_conv = EdgeConvBatch(gnn, output_size, batch_norm=True, dropout=0.2)\n",
    "    return graph_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNCaSlrFDGA6"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, x_input_size, adj_input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_x = nn.Sequential(\n",
    "            nn.Embedding(x_input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        if adj_input_size:\n",
    "            self.embed_adj = nn.Sequential(\n",
    "                nn.BatchNorm1d(adj_input_size),\n",
    "                nn.Linear(adj_input_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        else:\n",
    "            self.embed_adj = None\n",
    "\n",
    "        self.graph_conv_1 = get_graph_conv_layer((2 + bool(adj_input_size)) * hidden_size, 2 * hidden_size, hidden_size)\n",
    "        self.graph_conv_2 = get_graph_conv_layer(3 * hidden_size, 2 * hidden_size, hidden_size)\n",
    "        self.graph_conv_3 = get_graph_conv_layer(3 * hidden_size, 2 * hidden_size, hidden_size)\n",
    "        self.graph_conv_4 = get_graph_conv_layer(3 * hidden_size, 2 * hidden_size, hidden_size)\n",
    "        self.linear_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "\n",
    "        x = self.embed_x(x)\n",
    "        edge_index, _ = remove_self_loops(edge_index)  # We should remove self loops in this case!\n",
    "        edge_attr = self.embed_adj(edge_attr) if edge_attr is not None else None\n",
    "\n",
    "        x_out, edge_attr_out = self.graph_conv_1(x, edge_index, edge_attr)\n",
    "        x += x_out\n",
    "        edge_attr = (edge_attr + edge_attr_out) if edge_attr is not None else edge_attr_out\n",
    "\n",
    "        x = F.relu(x)\n",
    "        edge_attr = F.relu(edge_attr)\n",
    "        x_out, edge_attr_out = self.graph_conv_2(x, edge_index, edge_attr)\n",
    "        x += x_out\n",
    "        edge_attr += edge_attr_out\n",
    "\n",
    "        x = F.relu(x)\n",
    "        edge_attr = F.relu(edge_attr)\n",
    "        x_out, edge_attr_out = self.graph_conv_3(x, edge_index, edge_attr)\n",
    "        x += x_out\n",
    "        edge_attr += edge_attr_out\n",
    "\n",
    "        x = F.relu(x)\n",
    "        edge_attr = F.relu(edge_attr)\n",
    "        x_out, edge_attr_out = self.graph_conv_4(x, edge_index, edge_attr)\n",
    "        x += x_out\n",
    "        edge_attr += edge_attr_out\n",
    "\n",
    "        x = self.linear_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_fixed_width(lst, precision=None):\n",
    "    lst = [round(l, precision) if isinstance(l, float) else l for l in lst]\n",
    "    return [f\"{l: <18}\" for l in lst]\n",
    "\n",
    "\n",
    "class Stats:\n",
    "    epoch: int\n",
    "    step: int\n",
    "    batch_size: int\n",
    "    echo: bool\n",
    "    total_loss: float\n",
    "    num_correct_preds: int\n",
    "    num_preds: int\n",
    "    num_correct_preds_missing: int\n",
    "    num_preds_missing: int\n",
    "    num_correct_preds_missing_valid: int\n",
    "    num_preds_missing_valid: int\n",
    "    start_time: float\n",
    "\n",
    "    def __init__(self, *, epoch=0, step=0, batch_size=1, filename=None, echo=True):\n",
    "        self.epoch = epoch\n",
    "        self.step = step\n",
    "        self.batch_size = batch_size\n",
    "        self.echo = echo\n",
    "        self.reset_parameters()\n",
    "\n",
    "        if filename:\n",
    "            self.filehandle = open(filename, \"wt\", newline=\"\")\n",
    "            self.writer = csv.DictWriter(self.filehandle, list(self.stats.keys()), dialect=\"unix\")\n",
    "            self.writer.writeheader()\n",
    "            atexit.register(self.filehandle.close)\n",
    "        else:\n",
    "            self.filehandle = None\n",
    "            self.writer = None\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.num_steps = 0\n",
    "        self.total_loss = 0\n",
    "        self.num_correct_preds = 0\n",
    "        self.num_preds = 0\n",
    "        self.num_correct_preds_missing = 0\n",
    "        self.num_preds_missing = 0\n",
    "        self.num_correct_preds_missing_valid = 0\n",
    "        self.num_preds_missing_valid = 0\n",
    "        self.start_time = time.perf_counter()\n",
    "\n",
    "    @property\n",
    "    def header(self):\n",
    "        return \"\".join(to_fixed_width(self.stats.keys()))\n",
    "\n",
    "    @property\n",
    "    def row(self):\n",
    "        return \"\".join(to_fixed_width(self.stats.values(), 4))\n",
    "\n",
    "    @property\n",
    "    def stats(self):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return {\n",
    "                \"epoch\": self.epoch,\n",
    "                \"step\": self.step,\n",
    "                \"datapoint\": self.datapoint,\n",
    "                \"avg_loss\": np.float64(1) * self.total_loss / self.num_steps,\n",
    "                \"accuracy\": np.float64(1) * self.num_correct_preds / self.num_preds,\n",
    "                \"accuracy_m\": np.float64(1) * self.num_correct_preds_missing / self.num_preds_missing,\n",
    "                \"accuracy_mv\": self.accuracy_mv,\n",
    "                \"time_elapsed\": time.perf_counter() - self.start_time,\n",
    "            }\n",
    "\n",
    "    @property\n",
    "    def accuracy_mv(self):\n",
    "        return np.float64(1) * self.num_correct_preds_missing_valid / self.num_preds_missing_valid\n",
    "    \n",
    "    @property\n",
    "    def datapoint(self):\n",
    "        return self.step * self.batch_size\n",
    "\n",
    "    def write_header(self):\n",
    "        if self.echo:\n",
    "            print(self.header)\n",
    "        if self.writer is not None:\n",
    "            self.writer.writeheader()\n",
    "\n",
    "    def write_row(self):\n",
    "        if self.echo:\n",
    "            print(self.row)\n",
    "        if self.writer is not None:\n",
    "            self.writer.writerow(self.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "info_size = 2_000\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(sudoku_dataset_train, shuffle=True, num_workers=4, batch_size=batch_size, drop_last=True),\n",
    "    \"valid\": DataLoader(\n",
    "        sudoku_dataset_valid[:300], shuffle=False, num_workers=4, batch_size=batch_size, drop_last=False\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def get_stats_on_missing(x, y, output):\n",
    "    mask = (x == 9).squeeze()\n",
    "    if not mask.any():\n",
    "        return 0.0, 0.0\n",
    "    output_missing = output[mask]\n",
    "    _, predicted_missing = torch.max(output_missing.data, 1)\n",
    "    return (predicted_missing == y[mask]).sum().item(), len(predicted_missing)\n",
    "\n",
    "\n",
    "def get_data_x(y, frac_present, missing_val):\n",
    "    x = torch.where(\n",
    "        torch.rand(y.size(0), device=y.device) < frac_present,\n",
    "        y,\n",
    "        torch.tensor(missing_val, dtype=torch.long, device=y.device),\n",
    "    )\n",
    "    return x\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def eval_net(net: nn.Module):\n",
    "    training = net.training\n",
    "    try:\n",
    "        net.train(False)\n",
    "        yield\n",
    "    finally:\n",
    "        net.train(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6380
    },
    "colab_type": "code",
    "id": "azxPKTSKwcE8",
    "outputId": "09ed9b8b-25b9-49ff-8d02-83a4ec0ecba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch             step              datapoint         avg_loss          accuracy          accuracy_m        accuracy_mv       time_elapsed      \n",
      "0                 16                2048              1.502             0.4428            0.1796            0.4379            18.9025           \n",
      "0                 32                4096              0.8223            0.73              0.4248            0.5633            18.4664           \n",
      "0                 47                6016              1.2779            0.4914            0.2841            0.6493            17.4168           \n",
      "0                 63                8064              1.0953            0.5588            0.316             0.666             18.5926           \n",
      "0                 79                10112             0.8637            0.6411            0.3748            0.6937            18.6521           \n",
      "0                 94                12032             0.7579            0.6861            0.4065            0.6995            17.562            \n",
      "0                 110               14080             0.6206            0.7336            0.4936            0.7335            18.742            \n",
      "0                 125               16000             0.7659            0.6749            0.4317            0.7246            17.5629           \n",
      "0                 141               18048             1.2018            0.5011            0.315             0.7126            18.773            \n",
      "0                 157               20096             0.5987            0.7496            0.4785            0.7203            18.7235           \n",
      "0                 172               22016             0.7462            0.6785            0.4347            0.7323            17.6393           \n",
      "0                 188               24064             0.5479            0.7732            0.5192            0.7196            18.7241           \n",
      "0                 204               26112             0.7523            0.6835            0.4413            0.7417            18.7948           \n",
      "0                 219               28032             0.5942            0.7449            0.4929            0.7505            17.6596           \n",
      "0                 235               30080             0.7717            0.6782            0.4143            0.7331            18.8142           \n",
      "0                 250               32000             1.0417            0.5657            0.3361            0.7247            17.6294           \n",
      "0                 266               34048             0.9601            0.5998            0.3634            0.735             18.7953           \n",
      "0                 282               36096             0.742             0.6872            0.4491            0.7516            18.8188           \n",
      "0                 297               38016             0.5577            0.7617            0.5086            0.7465            17.6734           \n",
      "0                 313               40064             0.716             0.7057            0.4291            0.7418            18.8739           \n",
      "0                 329               42112             0.5555            0.763             0.5178            0.751             18.9646           \n",
      "0                 344               44032             0.7679            0.6766            0.4109            0.7559            17.7439           \n",
      "0                 360               46080             0.654             0.7163            0.4794            0.7572            18.8506           \n",
      "0                 375               48000             0.6813            0.7118            0.4675            0.7711            17.727            \n",
      "0                 391               50048             0.6561            0.7271            0.4339            0.7014            18.8423           \n",
      "0                 407               52096             0.5804            0.7525            0.5178            0.7535            18.8992           \n",
      "0                 422               54016             0.9429            0.6064            0.3829            0.7453            17.7094           \n",
      "0                 438               56064             0.7043            0.701             0.4538            0.7517            18.8877           \n",
      "0                 454               58112             1.1773            0.5119            0.3162            0.7607            18.9103           \n",
      "0                 469               60032             0.4496            0.8067            0.5706            0.7691            17.7522           \n",
      "0                 485               62080             0.8288            0.6531            0.4156            0.7622            18.9627           \n",
      "0                 500               64000             0.9171            0.6164            0.3883            0.7583            17.7292           \n",
      "0                 516               66048             0.3814            0.8341            0.605             0.764             18.8469           \n",
      "0                 532               68096             0.4559            0.8006            0.5605            0.7554            18.8919           \n",
      "0                 547               70016             0.5498            0.7695            0.5212            0.7802            17.5644           \n",
      "0                 563               72064             0.715             0.7048            0.4289            0.7704            18.8864           \n",
      "0                 579               74112             0.7067            0.7034            0.4426            0.7832            18.8532           \n",
      "0                 594               76032             0.7107            0.6989            0.478             0.7794            17.7089           \n",
      "0                 610               78080             0.7075            0.7012            0.4618            0.7819            18.9854           \n",
      "0                 625               80000             0.8841            0.6567            0.4118            0.7607            17.7203           \n",
      "0                 641               82048             0.687             0.7112            0.4713            0.778             18.8083           \n",
      "0                 657               84096             0.5884            0.7567            0.4964            0.7833            18.8468           \n",
      "0                 672               86016             0.4453            0.8134            0.5744            0.7804            17.7012           \n",
      "0                 688               88064             0.7178            0.7014            0.4555            0.7848            18.8441           \n",
      "0                 704               90112             0.6676            0.7197            0.4552            0.7896            18.9024           \n",
      "0                 719               92032             0.4279            0.8207            0.5742            0.7681            17.7411           \n",
      "0                 735               94080             0.7202            0.6955            0.4731            0.797             18.8585           \n",
      "0                 750               96000             0.6951            0.708             0.4644            0.787             17.7289           \n",
      "0                 766               98048             0.6732            0.7178            0.4894            0.7929            18.8329           \n",
      "0                 782               100096            0.617             0.7406            0.4745            0.7951            18.9499           \n",
      "0                 797               102016            0.756             0.6853            0.4104            0.7999            17.6971           \n",
      "0                 813               104064            0.6427            0.7308            0.494             0.7996            18.8546           \n",
      "0                 829               106112            0.6578            0.7212            0.4936            0.803             18.862            \n",
      "0                 844               108032            0.814             0.6586            0.4137            0.8014            17.7761           \n",
      "0                 860               110080            0.4776            0.7989            0.5131            0.788             18.8717           \n",
      "0                 875               112000            0.4205            0.8198            0.5864            0.7966            17.7523           \n",
      "0                 891               114048            0.4622            0.8011            0.5794            0.8032            18.875            \n",
      "0                 907               116096            0.5895            0.7662            0.5597            0.8113            18.917            \n",
      "0                 922               118016            0.8862            0.6336            0.3912            0.8023            17.7951           \n",
      "0                 938               120064            1.0458            0.5606            0.357             0.8129            18.9306           \n",
      "0                 954               122112            0.3947            0.8309            0.6078            0.8078            18.9257           \n",
      "0                 969               124032            0.5184            0.7849            0.5536            0.8131            17.7128           \n",
      "0                 985               126080            0.6462            0.7272            0.4858            0.8204            18.7851           \n",
      "0                 1000              128000            0.7226            0.6958            0.4595            0.825             17.803            \n",
      "0                 1016              130048            0.5115            0.786             0.5494            0.8258            18.9317           \n",
      "0                 1032              132096            0.554             0.7635            0.5519            0.8221            18.8358           \n",
      "0                 1047              134016            0.7365            0.6932            0.4588            0.8297            17.7884           \n",
      "0                 1063              136064            0.6155            0.7374            0.5327            0.8324            18.7539           \n",
      "0                 1079              138112            0.6355            0.7355            0.4805            0.8264            18.8611           \n",
      "0                 1094              140032            0.9249            0.6163            0.3974            0.8377            17.7222           \n",
      "0                 1110              142080            0.8926            0.6287            0.4162            0.8369            18.9898           \n",
      "0                 1125              144000            0.3516            0.8501            0.612             0.8408            17.6106           \n",
      "0                 1141              146048            0.8636            0.644             0.4338            0.8235            18.8615           \n",
      "0                 1157              148096            0.4894            0.7913            0.5835            0.8379            18.9344           \n",
      "0                 1172              150016            0.4239            0.8234            0.601             0.8452            17.7776           \n",
      "0                 1188              152064            0.8319            0.6407            0.4501            0.8523            18.8323           \n",
      "0                 1204              154112            0.6193            0.7392            0.5165            0.849             18.8488           \n",
      "0                 1219              156032            0.5138            0.7818            0.5651            0.8494            17.6779           \n",
      "0                 1235              158080            0.696             0.7126            0.4504            0.8551            18.7926           \n",
      "0                 1250              160000            1.1268            0.5338            0.3406            0.8552            17.7126           \n",
      "0                 1266              162048            0.5029            0.7876            0.5727            0.853             18.8641           \n",
      "0                 1282              164096            0.7831            0.6728            0.4522            0.8635            18.9026           \n",
      "0                 1297              166016            0.7347            0.6937            0.4622            0.8631            17.7365           \n",
      "0                 1313              168064            0.8429            0.6474            0.4315            0.8559            18.8744           \n",
      "0                 1329              170112            0.8527            0.6393            0.4215            0.8612            18.8837           \n",
      "0                 1344              172032            0.3433            0.8531            0.6355            0.8725            17.7034           \n",
      "0                 1360              174080            0.7384            0.6937            0.4612            0.8672            18.9108           \n",
      "0                 1375              176000            0.4898            0.7973            0.5796            0.8713            17.7387           \n",
      "0                 1391              178048            1.1039            0.5394            0.351             0.8566            18.8689           \n",
      "0                 1407              180096            0.5678            0.7629            0.5009            0.8634            18.8354           \n",
      "0                 1422              182016            0.5907            0.7544            0.513             0.8706            17.6994           \n",
      "0                 1438              184064            0.7227            0.6968            0.4698            0.8671            18.9278           \n",
      "0                 1454              186112            0.4147            0.8273            0.6453            0.8794            18.7698           \n",
      "0                 1469              188032            0.8874            0.6255            0.422             0.8709            17.763            \n",
      "0                 1485              190080            0.6943            0.7059            0.5053            0.8799            18.8392           \n",
      "0                 1500              192000            0.7545            0.6815            0.4589            0.8837            17.7392           \n",
      "0                 1516              194048            0.4888            0.7937            0.5742            0.8878            18.8512           \n",
      "0                 1532              196096            0.528             0.7782            0.5682            0.8843            18.8649           \n",
      "0                 1547              198016            0.6716            0.7154            0.475             0.8864            17.7603           \n",
      "0                 1563              200064            0.7308            0.6947            0.4643            0.8783            18.8601           \n",
      "0                 1579              202112            0.4435            0.8116            0.6058            0.8816            18.7732           \n",
      "0                 1594              204032            0.6132            0.7451            0.5019            0.8899            17.6919           \n",
      "0                 1610              206080            0.6786            0.7182            0.5111            0.8898            18.9328           \n",
      "0                 1625              208000            0.6038            0.7487            0.5395            0.8882            17.7367           \n",
      "0                 1641              210048            0.7294            0.6947            0.4674            0.8856            18.8535           \n",
      "0                 1657              212096            0.567             0.7642            0.5228            0.8778            18.828            \n",
      "0                 1672              214016            0.2914            0.8752            0.7034            0.8972            17.7195           \n",
      "0                 1688              216064            0.8221            0.6561            0.4387            0.879             18.8312           \n",
      "0                 1704              218112            0.7235            0.7017            0.4879            0.8915            18.8777           \n",
      "0                 1719              220032            0.5571            0.7658            0.5209            0.893             17.8147           \n",
      "0                 1735              222080            0.6893            0.7124            0.4948            0.8849            18.9006           \n",
      "0                 1750              224000            0.7296            0.6957            0.4706            0.8891            17.7792           \n",
      "0                 1766              226048            0.6968            0.7062            0.4919            0.8888            18.9063           \n",
      "0                 1782              228096            0.4772            0.8028            0.558             0.8881            18.9028           \n",
      "0                 1797              230016            0.9026            0.6232            0.4349            0.8864            17.7116           \n",
      "0                 1813              232064            0.5409            0.7696            0.5559            0.9023            18.7776           \n",
      "0                 1829              234112            0.7486            0.6876            0.4179            0.8852            18.8484           \n",
      "0                 1844              236032            0.5788            0.7564            0.5424            0.8971            17.7622           \n",
      "0                 1860              238080            0.6394            0.7354            0.4989            0.8941            18.9045           \n",
      "0                 1875              240000            0.4848            0.7988            0.5736            0.9003            17.6734           \n",
      "0                 1891              242048            0.3596            0.843             0.6619            0.9123            18.8629           \n",
      "0                 1907              244096            0.7623            0.6793            0.4789            0.8997            18.8369           \n",
      "0                 1922              246016            0.8821            0.6342            0.4123            0.8928            17.7272           \n",
      "0                 1938              248064            0.4982            0.7868            0.5781            0.9073            18.8405           \n",
      "0                 1954              250112            0.7533            0.6847            0.4812            0.9001            18.8543           \n",
      "0                 1969              252032            0.751             0.6811            0.4642            0.9049            17.7436           \n",
      "0                 1985              254080            0.5689            0.7616            0.5464            0.9122            18.9185           \n",
      "0                 2000              256000            0.6372            0.7331            0.5193            0.9016            17.6846           \n",
      "0                 2016              258048            0.6902            0.7096            0.5055            0.9093            18.7989           \n",
      "0                 2032              260096            0.5634            0.7624            0.528             0.9073            18.8268           \n",
      "0                 2047              262016            1.0201            0.5766            0.3858            0.8972            17.7684           \n",
      "0                 2063              264064            0.5098            0.7854            0.5661            0.9057            18.8518           \n",
      "0                 2079              266112            0.704             0.7047            0.4941            0.8989            18.7482           \n",
      "0                 2094              268032            0.6165            0.7431            0.5127            0.8935            17.6923           \n",
      "0                 2110              270080            0.2597            0.888             0.7188            0.9141            18.813            \n",
      "0                 2125              272000            0.7751            0.6705            0.457             0.9076            17.6871           \n",
      "0                 2141              274048            0.7772            0.6737            0.4669            0.8993            18.8847           \n",
      "0                 2157              276096            0.6774            0.7096            0.4854            0.9062            18.8016           \n",
      "0                 2172              278016            0.5091            0.7892            0.5737            0.9003            17.6323           \n",
      "0                 2188              280064            0.789             0.6681            0.4529            0.9037            18.8703           \n",
      "0                 2204              282112            0.693             0.7057            0.4925            0.91              18.746            \n",
      "0                 2219              284032            0.6523            0.7244            0.4742            0.9066            17.6674           \n",
      "0                 2235              286080            0.9123            0.6242            0.3942            0.9016            18.8643           \n",
      "0                 2250              288000            0.4087            0.8256            0.6318            0.9093            17.622            \n",
      "0                 2266              290048            0.7156            0.7013            0.4712            0.9066            18.888            \n",
      "0                 2282              292096            0.452             0.8102            0.6143            0.921             18.8429           \n",
      "0                 2297              294016            0.3451            0.8505            0.6717            0.9266            17.7086           \n",
      "0                 2313              296064            0.7306            0.6929            0.492             0.9124            18.9143           \n",
      "0                 2329              298112            0.2542            0.8889            0.7305            0.9288            18.8395           \n",
      "0                 2344              300032            0.823             0.6559            0.4635            0.9174            17.724            \n",
      "0                 2360              302080            0.8443            0.648             0.4357            0.9116            18.8647           \n",
      "0                 2375              304000            0.7043            0.7043            0.4856            0.9188            17.7282           \n",
      "0                 2391              306048            0.8061            0.6648            0.4319            0.9117            18.9097           \n",
      "0                 2407              308096            0.7945            0.6679            0.463             0.9155            18.8734           \n",
      "0                 2422              310016            0.3351            0.8552            0.6781            0.9282            17.6456           \n",
      "0                 2438              312064            0.7771            0.6739            0.4461            0.9181            18.9136           \n",
      "0                 2454              314112            1.0772            0.5509            0.3687            0.8985            18.9033           \n",
      "0                 2469              316032            0.4344            0.8145            0.615             0.9143            17.8081           \n",
      "0                 2485              318080            0.8495            0.645             0.4278            0.9025            18.8768           \n",
      "0                 2500              320000            0.3407            0.8558            0.6734            0.9252            17.7483           \n",
      "0                 2516              322048            0.6115            0.7442            0.5432            0.9229            18.8209           \n",
      "0                 2532              324096            0.5574            0.771             0.5022            0.9199            18.8479           \n",
      "0                 2547              326016            0.8339            0.6539            0.4307            0.9206            17.6877           \n",
      "0                 2563              328064            0.7558            0.6845            0.4685            0.9169            18.8869           \n",
      "0                 2579              330112            0.574             0.7584            0.5421            0.9165            18.7652           \n",
      "0                 2594              332032            0.7705            0.6835            0.4447            0.9124            17.7328           \n",
      "0                 2610              334080            0.7945            0.67              0.4689            0.9109            18.8639           \n",
      "0                 2625              336000            0.7041            0.7081            0.4692            0.9163            17.6619           \n",
      "0                 2641              338048            0.4933            0.7906            0.5886            0.9247            18.9038           \n",
      "0                 2657              340096            0.763             0.6787            0.4807            0.9162            18.9288           \n",
      "0                 2672              342016            0.6159            0.7348            0.5245            0.9304            17.7555           \n",
      "0                 2688              344064            0.677             0.713             0.4955            0.9276            18.8715           \n",
      "0                 2704              346112            0.6736            0.7181            0.5107            0.9258            18.8276           \n",
      "0                 2719              348032            0.5973            0.7485            0.5286            0.93              17.7206           \n",
      "0                 2735              350080            0.989             0.5891            0.3777            0.9155            18.8653           \n",
      "0                 2750              352000            0.6474            0.73              0.5242            0.9196            17.7681           \n",
      "0                 2766              354048            0.3804            0.8376            0.6663            0.9262            18.7741           \n",
      "0                 2782              356096            0.7539            0.6756            0.4654            0.9321            18.8117           \n",
      "0                 2797              358016            1.0128            0.583             0.3578            0.9095            17.7441           \n",
      "0                 2813              360064            0.654             0.7284            0.4852            0.9238            18.9316           \n",
      "0                 2829              362112            0.5664            0.7648            0.5689            0.9299            18.7847           \n",
      "0                 2844              364032            0.7351            0.691             0.4816            0.9259            17.7126           \n",
      "0                 2860              366080            0.6334            0.7395            0.5142            0.9251            18.8959           \n",
      "0                 2875              368000            0.5141            0.7813            0.5842            0.9333            17.7564           \n",
      "0                 2891              370048            0.4735            0.7993            0.5735            0.9321            18.9035           \n",
      "0                 2907              372096            0.4794            0.7996            0.5931            0.9327            18.8807           \n",
      "0                 2922              374016            0.3554            0.8463            0.6632            0.9449            17.7785           \n",
      "0                 2938              376064            0.6787            0.7145            0.5099            0.9278            18.7716           \n",
      "0                 2954              378112            0.7329            0.6877            0.462             0.9256            18.8712           \n",
      "0                 2969              380032            0.74              0.6822            0.4852            0.929             17.7125           \n",
      "0                 2985              382080            0.6997            0.7059            0.5153            0.927             18.7972           \n",
      "0                 3000              384000            0.6802            0.7187            0.5004            0.9199            17.7326           \n",
      "0                 3016              386048            0.6847            0.7145            0.4746            0.9273            18.8917           \n",
      "0                 3032              388096            0.5676            0.7626            0.5172            0.9283            18.8042           \n",
      "0                 3047              390016            0.4301            0.8231            0.623             0.9289            17.6844           \n",
      "0                 3063              392064            0.7248            0.6925            0.484             0.9293            18.9413           \n",
      "0                 3079              394112            0.6517            0.7265            0.514             0.9332            18.8449           \n",
      "0                 3094              396032            0.6882            0.7036            0.499             0.9359            17.7397           \n",
      "0                 3110              398080            0.7022            0.7136            0.4602            0.926             18.7981           \n",
      "0                 3125              400000            0.5856            0.7521            0.528             0.9374            17.6735           \n",
      "0                 3141              402048            0.6771            0.7189            0.4874            0.9345            18.8136           \n",
      "0                 3157              404096            0.4828            0.792             0.5734            0.9446            18.9141           \n",
      "0                 3172              406016            0.4528            0.8012            0.6205            0.9466            17.6913           \n",
      "0                 3188              408064            0.6704            0.7204            0.5196            0.9309            18.8217           \n",
      "0                 3204              410112            0.523             0.7813            0.5875            0.9418            18.8217           \n",
      "0                 3219              412032            0.6041            0.7475            0.5266            0.937             17.74             \n",
      "0                 3235              414080            0.5778            0.7581            0.5336            0.9404            18.8147           \n",
      "0                 3250              416000            0.4837            0.7938            0.5536            0.9436            17.7252           \n",
      "0                 3266              418048            0.6696            0.7162            0.5238            0.9431            18.8362           \n",
      "0                 3282              420096            0.6306            0.7359            0.5203            0.9342            18.881            \n",
      "0                 3297              422016            0.7927            0.6697            0.446             0.9222            17.7435           \n",
      "0                 3313              424064            0.6676            0.7127            0.5112            0.9365            18.8309           \n",
      "0                 3329              426112            0.8482            0.6421            0.4355            0.9367            18.9279           \n",
      "0                 3344              428032            0.4466            0.8145            0.5823            0.9384            17.644            \n",
      "0                 3360              430080            0.6972            0.6998            0.5121            0.9459            18.8586           \n",
      "0                 3375              432000            0.6705            0.7121            0.5144            0.9444            17.6777           \n",
      "0                 3391              434048            0.477             0.7952            0.5954            0.9502            18.8273           \n",
      "0                 3407              436096            0.3339            0.8618            0.6898            0.9494            18.8368           \n",
      "0                 3422              438016            0.439             0.8112            0.5976            0.947             17.7              \n",
      "0                 3438              440064            0.6665            0.7187            0.5146            0.9442            18.838            \n",
      "0                 3454              442112            0.7038            0.7039            0.5012            0.9376            18.8607           \n",
      "0                 3469              444032            0.8303            0.6521            0.4424            0.9391            17.7584           \n",
      "0                 3485              446080            0.861             0.6404            0.4467            0.9339            18.8431           \n",
      "0                 3500              448000            0.6001            0.746             0.5347            0.9429            17.6837           \n",
      "0                 3516              450048            0.8237            0.6637            0.4306            0.9286            18.9168           \n",
      "0                 3532              452096            0.4911            0.791             0.5853            0.944             18.8507           \n",
      "0                 3547              454016            0.6825            0.7131            0.5094            0.9451            17.7418           \n",
      "0                 3563              456064            0.2394            0.8939            0.7403            0.9535            18.8757           \n",
      "0                 3579              458112            0.6578            0.7267            0.504             0.9417            18.8122           \n",
      "0                 3594              460032            0.5896            0.7506            0.5137            0.9478            17.6037           \n",
      "0                 3610              462080            0.6383            0.7294            0.5243            0.9489            18.8714           \n",
      "0                 3625              464000            0.4644            0.8044            0.6258            0.9469            17.6975           \n",
      "0                 3641              466048            0.9056            0.6224            0.426             0.9422            18.8439           \n",
      "0                 3657              468096            0.6467            0.7264            0.5358            0.9458            18.8204           \n",
      "0                 3672              470016            0.5826            0.7525            0.5708            0.9475            17.7725           \n",
      "0                 3688              472064            0.686             0.7101            0.4876            0.9468            18.854            \n",
      "0                 3704              474112            0.7777            0.6767            0.4565            0.9458            18.8899           \n",
      "0                 3719              476032            0.6407            0.7266            0.5348            0.9483            17.6663           \n",
      "0                 3735              478080            0.8404            0.6503            0.4212            0.9399            18.8609           \n",
      "0                 3750              480000            0.3066            0.8699            0.6873            0.9526            17.6871           \n",
      "0                 3766              482048            0.3836            0.8345            0.6326            0.9535            18.8154           \n",
      "0                 3782              484096            0.8911            0.6273            0.4006            0.9453            18.8817           \n",
      "0                 3797              486016            0.3874            0.8326            0.6395            0.9531            17.7234           \n",
      "0                 3813              488064            0.4422            0.809             0.5984            0.959             18.8345           \n",
      "0                 3829              490112            0.7458            0.6856            0.4607            0.9463            18.868            \n",
      "0                 3844              492032            0.7267            0.6948            0.4492            0.9483            17.6878           \n",
      "0                 3860              494080            0.6899            0.7104            0.5018            0.9445            18.8338           \n",
      "0                 3875              496000            0.7171            0.6886            0.4964            0.9517            17.6361           \n",
      "0                 3891              498048            0.4638            0.8056            0.6169            0.9523            18.8271           \n",
      "0                 3907              500096            0.7287            0.6955            0.4632            0.9482            18.8571           \n",
      "0                 3922              502016            0.6666            0.7236            0.5137            0.9465            17.6969           \n",
      "0                 3938              504064            0.4935            0.7959            0.5911            0.9527            18.8152           \n",
      "0                 3954              506112            0.4258            0.8214            0.5945            0.9491            18.8425           \n",
      "0                 3969              508032            0.619             0.7374            0.5105            0.9498            17.5898           \n",
      "0                 3985              510080            0.6603            0.7212            0.519             0.9513            18.8064           \n",
      "0                 4000              512000            0.9196            0.6165            0.4201            0.9426            17.747            \n",
      "0                 4016              514048            0.4288            0.8133            0.6342            0.9562            18.7829           \n",
      "0                 4032              516096            0.478             0.8013            0.5556            0.9483            18.7998           \n",
      "0                 4047              518016            0.8582            0.6427            0.4588            0.9414            17.7358           \n",
      "0                 4063              520064            0.4757            0.7994            0.5828            0.9515            18.8132           \n",
      "0                 4079              522112            0.4601            0.8041            0.6037            0.9524            18.8537           \n",
      "0                 4094              524032            0.8445            0.6454            0.4646            0.952             17.7261           \n",
      "0                 4110              526080            0.3958            0.8294            0.6284            0.9525            18.8721           \n",
      "0                 4125              528000            0.5309            0.7769            0.5366            0.9536            17.6964           \n",
      "0                 4141              530048            0.7536            0.6868            0.4414            0.9434            18.8756           \n",
      "0                 4157              532096            0.5278            0.7747            0.5657            0.9531            18.8528           \n",
      "0                 4172              534016            0.6255            0.7455            0.4605            0.9447            17.6378           \n",
      "0                 4188              536064            0.3854            0.8342            0.6533            0.9567            18.9074           \n",
      "0                 4204              538112            0.4988            0.7917            0.55              0.9559            18.7769           \n",
      "0                 4219              540032            0.5325            0.7732            0.5675            0.9562            17.7013           \n",
      "0                 4235              542080            0.4423            0.8144            0.5935            0.9531            18.8273           \n",
      "0                 4250              544000            0.6894            0.71              0.505             0.9516            17.7212           \n",
      "0                 4266              546048            0.3848            0.8329            0.6804            0.9636            18.868            \n",
      "0                 4282              548096            0.5959            0.7463            0.5579            0.9556            18.9169           \n",
      "0                 4297              550016            0.5434            0.7743            0.5471            0.9538            17.7228           \n",
      "0                 4313              552064            0.4399            0.8135            0.6242            0.9539            18.8488           \n",
      "0                 4329              554112            0.6648            0.7174            0.5242            0.9532            18.9135           \n",
      "0                 4344              556032            0.4186            0.8198            0.6215            0.958             17.7859           \n",
      "0                 4360              558080            0.7594            0.6845            0.4367            0.944             18.736            \n",
      "0                 4375              560000            0.3113            0.8656            0.7029            0.9621            17.7149           \n",
      "0                 4391              562048            0.6945            0.7071            0.5057            0.958             18.7901           \n",
      "0                 4407              564096            0.3707            0.8408            0.6588            0.9618            18.8647           \n",
      "0                 4422              566016            0.3019            0.8679            0.7165            0.9645            17.6872           \n",
      "0                 4438              568064            0.5788            0.7591            0.5465            0.9543            18.8342           \n",
      "0                 4454              570112            0.8237            0.6569            0.4168            0.9508            18.8229           \n",
      "0                 4469              572032            0.9417            0.6098            0.3903            0.9419            17.7168           \n",
      "0                 4485              574080            0.5415            0.7722            0.527             0.9481            18.7472           \n",
      "0                 4500              576000            0.3985            0.831             0.6318            0.9592            17.7053           \n",
      "0                 4516              578048            0.6375            0.7325            0.5111            0.9504            19.0813           \n",
      "0                 4532              580096            0.5396            0.7676            0.5774            0.9613            18.7011           \n",
      "0                 4547              582016            0.5653            0.7567            0.5953            0.9617            17.6473           \n",
      "0                 4563              584064            0.5356            0.7767            0.5437            0.9601            18.7504           \n",
      "0                 4579              586112            0.8201            0.6617            0.4232            0.9419            18.6821           \n",
      "0                 4594              588032            0.5605            0.7629            0.5559            0.9544            17.6291           \n",
      "0                 4610              590080            0.6861            0.7133            0.5023            0.9537            18.9398           \n",
      "0                 4625              592000            0.443             0.8096            0.6208            0.9634            17.8512           \n",
      "0                 4641              594048            0.2023            0.9115            0.7641            0.9675            18.7191           \n",
      "0                 4657              596096            0.759             0.6833            0.464             0.9559            18.6588           \n",
      "0                 4672              598016            0.4183            0.8199            0.6324            0.9593            17.6368           \n",
      "0                 4688              600064            0.3659            0.8439            0.6605            0.9594            18.7409           \n",
      "0                 4704              602112            0.6183            0.7391            0.5312            0.9602            18.7339           \n",
      "0                 4719              604032            0.3821            0.8389            0.6435            0.9602            17.5907           \n",
      "0                 4735              606080            0.729             0.694             0.483             0.947             18.7731           \n",
      "0                 4750              608000            0.5308            0.7822            0.5684            0.9495            17.6282           \n",
      "0                 4766              610048            0.5659            0.7606            0.5521            0.9571            18.8056           \n",
      "0                 4782              612096            0.5052            0.7874            0.5749            0.9645            18.7554           \n",
      "0                 4797              614016            0.9429            0.611             0.3738            0.9564            17.5335           \n",
      "0                 4813              616064            0.832             0.6549            0.4441            0.9538            18.811            \n",
      "0                 4829              618112            0.5193            0.78              0.5693            0.954             18.8456           \n",
      "0                 4844              620032            0.9272            0.6185            0.391             0.947             17.6304           \n",
      "0                 4860              622080            0.5102            0.7839            0.5726            0.9556            18.7258           \n",
      "0                 4875              624000            0.5483            0.7686            0.5864            0.9624            17.5865           \n",
      "0                 4891              626048            0.5559            0.7648            0.577             0.9583            18.7927           \n",
      "0                 4907              628096            0.36              0.8441            0.6713            0.9671            18.7239           \n",
      "0                 4922              630016            0.5795            0.7576            0.5091            0.9589            17.6594           \n",
      "0                 4938              632064            0.5858            0.7537            0.539             0.9567            18.8287           \n",
      "0                 4954              634112            0.65              0.7254            0.5178            0.9583            19.0516           \n",
      "0                 4969              636032            0.5344            0.775             0.5487            0.9592            17.8375           \n",
      "0                 4985              638080            0.6506            0.7267            0.4981            0.959             18.8117           \n",
      "0                 5000              640000            0.8536            0.6451            0.4285            0.9597            17.588            \n",
      "0                 5016              642048            0.4172            0.826             0.6099            0.9653            18.7377           \n",
      "0                 5032              644096            0.3382            0.8549            0.6837            0.9674            18.7793           \n",
      "0                 5047              646016            0.7341            0.6946            0.465             0.9529            17.6227           \n",
      "0                 5063              648064            0.9222            0.6138            0.3881            0.9535            18.8051           \n",
      "0                 5079              650112            0.3209            0.8611            0.6657            0.9645            18.8              \n",
      "0                 5094              652032            0.7688            0.6785            0.4708            0.9585            17.6338           \n",
      "0                 5110              654080            0.3006            0.867             0.7224            0.9689            18.8136           \n",
      "0                 5125              656000            0.3994            0.8292            0.6611            0.9695            17.6142           \n",
      "0                 5141              658048            0.6818            0.7099            0.512             0.9595            18.7992           \n",
      "0                 5157              660096            0.5114            0.7827            0.5733            0.9638            18.7262           \n",
      "0                 5172              662016            0.6399            0.735             0.5078            0.9614            17.6916           \n",
      "0                 5188              664064            0.5334            0.7685            0.5709            0.9678            18.7482           \n",
      "0                 5204              666112            0.9753            0.5943            0.3698            0.9564            18.7991           \n",
      "0                 5219              668032            0.5259            0.7792            0.5992            0.962             17.6021           \n",
      "0                 5235              670080            0.7275            0.7001            0.4793            0.9456            18.8035           \n",
      "0                 5250              672000            0.5909            0.7521            0.5469            0.9595            17.6825           \n",
      "0                 5266              674048            0.6619            0.7243            0.4943            0.9542            18.7567           \n",
      "0                 5282              676096            0.6224            0.7375            0.5234            0.9622            18.8362           \n",
      "0                 5297              678016            0.7226            0.7004            0.4738            0.9554            17.6712           \n",
      "0                 5313              680064            0.3401            0.8514            0.6785            0.9706            18.7547           \n",
      "0                 5329              682112            0.776             0.6726            0.4794            0.9605            18.8691           \n",
      "0                 5344              684032            0.9567            0.5987            0.3974            0.9523            17.5804           \n",
      "0                 5360              686080            0.4772            0.7931            0.6252            0.9673            18.8162           \n",
      "0                 5375              688000            0.6452            0.7283            0.5068            0.9648            17.7273           \n",
      "0                 5391              690048            0.6979            0.7147            0.4754            0.9589            18.822            \n",
      "0                 5407              692096            0.8597            0.6462            0.4457            0.9588            18.7812           \n",
      "0                 5422              694016            0.5908            0.7475            0.5314            0.9617            17.5975           \n",
      "0                 5438              696064            0.5491            0.7678            0.5612            0.9677            18.7565           \n",
      "0                 5454              698112            0.5759            0.7618            0.5502            0.9588            18.7291           \n",
      "1                 5469              700032            0.3676            0.8398            0.6471            0.9685            17.9828           \n",
      "1                 5485              702080            0.6086            0.749             0.5034            0.9628            18.8012           \n",
      "1                 5500              704000            0.7231            0.6961            0.4971            0.9638            17.8074           \n",
      "1                 5516              706048            0.426             0.8195            0.6431            0.9682            18.8614           \n",
      "1                 5532              708096            0.4685            0.8048            0.6038            0.965             18.8032           \n",
      "1                 5547              710016            0.7219            0.6998            0.4668            0.9581            17.7134           \n",
      "1                 5563              712064            0.4325            0.8116            0.6431            0.9657            18.7862           \n",
      "1                 5579              714112            0.5396            0.7755            0.5372            0.9686            18.8043           \n",
      "1                 5594              716032            0.6787            0.7139            0.516             0.9633            17.6231           \n",
      "1                 5610              718080            0.2532            0.8895            0.7246            0.9714            18.7923           \n",
      "1                 5625              720000            0.6883            0.7146            0.4955            0.9588            17.6438           \n",
      "1                 5641              722048            0.8595            0.6447            0.4135            0.9578            18.7593           \n",
      "1                 5657              724096            0.3698            0.8432            0.679             0.9699            18.8172           \n",
      "1                 5672              726016            0.5596            0.7665            0.5663            0.9671            17.6894           \n",
      "1                 5688              728064            0.6934            0.7095            0.5157            0.9647            18.8326           \n",
      "1                 5704              730112            0.5342            0.7755            0.552             0.9669            18.758            \n",
      "1                 5719              732032            0.6515            0.7266            0.5074            0.9583            17.6525           \n",
      "1                 5735              734080            0.9792            0.5917            0.3977            0.9566            18.8076           \n",
      "1                 5750              736000            0.5231            0.7755            0.5776            0.9698            17.6563           \n",
      "1                 5766              738048            0.5576            0.7603            0.5652            0.97              18.8189           \n",
      "1                 5782              740096            0.5646            0.76              0.5646            0.9709            18.8166           \n",
      "1                 5797              742016            0.5077            0.7884            0.5694            0.9699            17.6058           \n",
      "1                 5813              744064            0.3625            0.8472            0.6502            0.9708            18.7896           \n",
      "1                 5829              746112            0.7096            0.7026            0.4694            0.9677            18.78             \n",
      "1                 5844              748032            0.8666            0.6373            0.4338            0.9564            17.6313           \n",
      "1                 5860              750080            0.8659            0.638             0.3899            0.956             18.8009           \n",
      "1                 5875              752000            0.552             0.7681            0.5511            0.9594            17.677            \n",
      "1                 5891              754048            0.5919            0.7488            0.5421            0.9638            18.8062           \n",
      "1                 5907              756096            0.5105            0.784             0.5879            0.9631            18.772            \n",
      "1                 5922              758016            0.3818            0.8363            0.6589            0.97              17.6893           \n",
      "1                 5938              760064            0.7499            0.6868            0.4601            0.9628            18.8291           \n",
      "1                 5954              762112            0.8318            0.6496            0.4596            0.9624            18.8157           \n",
      "1                 5969              764032            0.8814            0.6294            0.4278            0.9633            17.6451           \n",
      "1                 5985              766080            0.5428            0.7699            0.548             0.9669            18.7684           \n",
      "1                 6000              768000            0.4451            0.806             0.6188            0.9736            17.6285           \n",
      "1                 6016              770048            0.5292            0.7771            0.5597            0.9698            18.7073           \n",
      "1                 6032              772096            0.4993            0.7864            0.5928            0.9713            18.7212           \n",
      "1                 6047              774016            0.411             0.8266            0.6391            0.9706            17.6309           \n",
      "1                 6063              776064            0.393             0.832             0.6458            0.9728            18.7842           \n",
      "1                 6079              778112            0.4343            0.8146            0.6021            0.9713            18.7905           \n",
      "1                 6094              780032            0.3871            0.8345            0.639             0.9734            17.6931           \n",
      "1                 6110              782080            0.5992            0.7484            0.5167            0.9684            18.7997           \n",
      "1                 6125              784000            0.7592            0.683             0.4867            0.9674            17.6992           \n",
      "1                 6141              786048            0.6749            0.7208            0.4737            0.9531            18.744            \n",
      "1                 6157              788096            0.4595            0.8078            0.6147            0.9631            18.7791           \n",
      "1                 6172              790016            0.6509            0.7269            0.5292            0.9617            17.6878           \n",
      "1                 6188              792064            0.7035            0.7089            0.5029            0.9616            18.8316           \n",
      "1                 6204              794112            0.7751            0.6766            0.4709            0.9626            18.7945           \n",
      "1                 6219              796032            0.4792            0.7974            0.5783            0.9698            17.6704           \n",
      "1                 6235              798080            0.4738            0.8               0.6116            0.9725            18.7635           \n",
      "1                 6250              800000            0.5844            0.7531            0.5472            0.9694            17.6699           \n",
      "1                 6266              802048            0.262             0.8835            0.7267            0.9768            18.7748           \n",
      "1                 6282              804096            0.5757            0.7576            0.5513            0.9674            18.7398           \n",
      "1                 6297              806016            0.604             0.7443            0.5631            0.9704            17.6644           \n",
      "1                 6313              808064            0.6125            0.7424            0.5208            0.9694            18.7812           \n",
      "1                 6329              810112            0.6164            0.7365            0.535             0.9722            18.7654           \n",
      "1                 6344              812032            0.4947            0.7891            0.6035            0.971             17.6452           \n",
      "1                 6360              814080            0.9003            0.6253            0.4376            0.9668            18.7555           \n",
      "1                 6375              816000            0.7302            0.6984            0.4691            0.9607            17.7067           \n",
      "1                 6391              818048            0.6076            0.7452            0.5424            0.9689            18.8011           \n",
      "1                 6407              820096            0.734             0.6946            0.4845            0.964             18.8465           \n",
      "1                 6422              822016            0.6009            0.7441            0.5308            0.9698            17.6662           \n",
      "1                 6438              824064            0.6675            0.7191            0.5022            0.9676            18.7948           \n",
      "1                 6454              826112            0.5236            0.7823            0.5806            0.9718            18.8126           \n",
      "1                 6469              828032            0.7742            0.6798            0.4403            0.9606            17.7017           \n",
      "1                 6485              830080            0.4301            0.8144            0.6432            0.9731            18.8117           \n",
      "1                 6500              832000            0.6416            0.7305            0.514             0.9688            17.6965           \n",
      "1                 6516              834048            0.9384            0.608             0.4165            0.9582            18.8288           \n",
      "1                 6532              836096            0.6719            0.7175            0.4862            0.9657            18.7842           \n",
      "1                 6547              838016            0.4322            0.815             0.6575            0.9746            17.5688           \n",
      "1                 6563              840064            0.5021            0.7913            0.5719            0.9676            18.7532           \n",
      "1                 6579              842112            0.5112            0.7814            0.5943            0.975             18.7778           \n",
      "1                 6594              844032            0.4082            0.824             0.6603            0.9722            17.6074           \n",
      "1                 6610              846080            0.7598            0.6859            0.4835            0.9648            18.7651           \n",
      "1                 6625              848000            0.5585            0.7628            0.5697            0.9705            17.5743           \n",
      "1                 6641              850048            0.5546            0.766             0.5721            0.9677            18.8026           \n",
      "1                 6657              852096            0.6722            0.7207            0.4794            0.9669            18.7394           \n",
      "1                 6672              854016            0.8495            0.6425            0.4493            0.9683            17.5495           \n",
      "1                 6688              856064            0.5215            0.7781            0.5822            0.973             18.7111           \n",
      "1                 6704              858112            0.515             0.7816            0.5835            0.973             18.7369           \n",
      "1                 6719              860032            0.4026            0.825             0.6618            0.9768            17.6122           \n",
      "1                 6735              862080            0.6851            0.7071            0.4815            0.9701            18.7294           \n",
      "1                 6750              864000            0.3781            0.8334            0.6705            0.9758            17.5689           \n",
      "1                 6766              866048            0.7238            0.697             0.4924            0.9677            18.8161           \n",
      "1                 6782              868096            0.6201            0.7396            0.5415            0.969             18.8004           \n",
      "1                 6797              870016            0.6861            0.7153            0.5077            0.9654            17.6525           \n",
      "1                 6813              872064            0.5051            0.7827            0.5813            0.9741            18.7058           \n",
      "1                 6829              874112            0.4604            0.8066            0.5828            0.9761            18.6993           \n",
      "1                 6844              876032            0.4251            0.8228            0.6426            0.9737            17.6557           \n",
      "1                 6860              878080            0.2651            0.8851            0.7513            0.9787            18.7666           \n",
      "1                 6875              880000            0.651             0.7218            0.5205            0.977             17.6819           \n",
      "1                 6891              882048            0.3914            0.8301            0.6322            0.9784            18.716            \n",
      "1                 6907              884096            0.8316            0.6512            0.474             0.9702            18.7304           \n",
      "1                 6922              886016            0.6348            0.7296            0.4935            0.974             17.6355           \n",
      "1                 6938              888064            0.6845            0.7114            0.4844            0.9727            18.7697           \n",
      "1                 6954              890112            0.6422            0.7272            0.5194            0.9717            18.7913           \n",
      "1                 6969              892032            0.8341            0.6465            0.4651            0.9696            17.631            \n",
      "1                 6985              894080            0.6456            0.7291            0.4995            0.9689            18.7606           \n",
      "1                 7000              896000            0.5147            0.7836            0.5858            0.973             17.5992           \n",
      "1                 7016              898048            0.6746            0.7163            0.4968            0.9729            18.7328           \n",
      "1                 7032              900096            0.5771            0.7588            0.5469            0.9667            18.6843           \n",
      "1                 7047              902016            0.5977            0.749             0.5236            0.9718            17.617            \n",
      "1                 7063              904064            0.8914            0.6274            0.4385            0.9655            18.7222           \n",
      "1                 7079              906112            0.5866            0.7565            0.5196            0.9682            18.7242           \n",
      "1                 7094              908032            0.4447            0.8096            0.624             0.9756            17.5992           \n",
      "1                 7110              910080            0.8205            0.6565            0.4158            0.9662            18.7193           \n",
      "1                 7125              912000            0.405             0.8275            0.6442            0.977             17.5617           \n",
      "1                 7141              914048            0.5881            0.75              0.5568            0.9766            18.7285           \n",
      "1                 7157              916096            0.762             0.6815            0.4704            0.9708            18.7842           \n",
      "1                 7172              918016            1.0107            0.5812            0.3588            0.9631            17.609            \n",
      "1                 7188              920064            0.5303            0.7721            0.588             0.9769            18.6554           \n",
      "1                 7204              922112            0.6577            0.7262            0.4885            0.9689            18.7757           \n",
      "1                 7219              924032            1.0499            0.5631            0.3995            0.9643            17.6609           \n",
      "1                 7235              926080            0.7612            0.6875            0.4727            0.9605            18.7435           \n",
      "1                 7250              928000            0.4603            0.8044            0.617             0.9751            17.553            \n",
      "1                 7266              930048            0.6328            0.7324            0.5175            0.9701            18.7874           \n",
      "1                 7282              932096            0.7023            0.6999            0.5185            0.9683            18.9458           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/kimlab1/strokach/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d551246457ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "continue_previous = False\n",
    "\n",
    "if not continue_previous:\n",
    "    net = Net(x_input_size=10, adj_input_size=None, hidden_size=192, output_size=9).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "    stats = Stats(epoch=0, step=0, batch_size=batch_size, filename=NOTEBOOK_PATH / \"training.log\", echo=True)\n",
    "    stats.write_header()\n",
    "\n",
    "m = torch.distributions.uniform.Uniform(0, 0.9)\n",
    "\n",
    "net = net.train()\n",
    "for epoch in range(stats.epoch + 1 if continue_previous else 0, 100_000):\n",
    "    stats.epoch = epoch\n",
    "    for data in dataloaders[\"train\"]:\n",
    "        stats.step += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = data.to(device)\n",
    "        x = get_data_x(data.y, m.sample().to(device), 9)\n",
    "        output = net(x, data.edge_index, data.edge_attr if hasattr(data, \"edge_attr\") else None)\n",
    "\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "\n",
    "        stats.total_loss += loss.detach().item()\n",
    "        stats.num_steps += 1\n",
    "\n",
    "        # Accuracy for all\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        stats.num_correct_preds += (predicted == data.y).sum().item()\n",
    "        stats.num_preds += len(predicted)\n",
    "\n",
    "        # Accuracy for missing only\n",
    "        num_correct, num_total = get_stats_on_missing(x, data.y, output)\n",
    "        stats.num_correct_preds_missing += num_correct\n",
    "        stats.num_preds_missing += num_total\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (stats.datapoint % info_size) < batch_size:\n",
    "            for j, data in enumerate(dataloaders[\"valid\"]):\n",
    "                data = data.to(device)\n",
    "\n",
    "                with torch.no_grad() and eval_net(net):\n",
    "                    output = net(data.x, data.edge_index, data.edge_attr if hasattr(data, \"edge_attr\") else None)\n",
    "\n",
    "                num_correct, num_total = get_stats_on_missing(data.x, data.y, output)\n",
    "                stats.num_correct_preds_missing_valid += num_correct\n",
    "                stats.num_preds_missing_valid += num_total\n",
    "\n",
    "            stats.write_row()\n",
    "            stats.reset_parameters()\n",
    "    output_filename = f\"e{stats.epoch}-s{stats.step}-d{stats.datapoint}.state\"\n",
    "    torch.save(net.state_dict(), NOTEBOOK_PATH.joinpath(output_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1fXJz1l9HvhW",
    "u9q4iTifHvhl"
   ],
   "name": "Copy of 2019-03-30-sudoku-4xEdgeConv-09862+.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
