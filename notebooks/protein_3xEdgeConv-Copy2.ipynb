{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_gnhLD0HvhU",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fXJz1l9HvhW"
   },
   "source": [
    "### Install `pytorch_geometric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "hIbvKDk_HvhX",
    "outputId": "349cb6d3-ffbc-4550-d9dd-dd6288ed5356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-scatter\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/83/67eeea00c2db1959e2ff95d8680dbd756977bfab254bda8658f09dc3bc11/torch_scatter-1.1.2.tar.gz\n",
      "Building wheels for collected packages: torch-scatter\n",
      "  Building wheel for torch-scatter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a8/41/ef/1a52be728eedba23aa6d39b0bd6e9b533cb7ff572e967a6a43\n",
      "Successfully built torch-scatter\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "tw-y9MmIHvha",
    "outputId": "0666864a-5017-40a1-a77c-d23388eb0b1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-sparse\n",
      "  Downloading https://files.pythonhosted.org/packages/73/72/e374662f6f47d9ac0e082a6d5c18d14e15c52863e89c6bc6957a0d2ed026/torch_sparse-0.2.4.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse) (1.14.6)\n",
      "Building wheels for collected packages: torch-sparse\n",
      "  Building wheel for torch-sparse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/14/8f/77/7be0a2e42c5d1a6257b71edf5dc42f9e148d2137b66d869ee0\n",
      "Successfully built torch-sparse\n",
      "Installing collected packages: torch-sparse\n",
      "Successfully installed torch-sparse-0.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "n3SMtaYHHvhc",
    "outputId": "6250a80b-b32d-4fc8-cf90-ad0507e83ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-cluster\n",
      "  Downloading https://files.pythonhosted.org/packages/32/c8/9b3af10be647326dd807bb2fe7ced8ae4c3fd74178dba884621749afc4d7/torch_cluster-1.2.4.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from torch-cluster) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-cluster) (1.14.6)\n",
      "Building wheels for collected packages: torch-cluster\n",
      "  Building wheel for torch-cluster (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/5b/54/4a/ae0125851936c2b5ea13dddb1ab4b103b9f71e0f3211a6c4e1\n",
      "Successfully built torch-cluster\n",
      "Installing collected packages: torch-cluster\n",
      "Successfully installed torch-cluster-1.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "4AQ_A4XLHvhg",
    "outputId": "601b9d38-8291-431d-a90b-046bb233469a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-spline-conv\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/6d/b34721af4bb907814a41a1e0e5e426c97aee644efef6877ed112bfb3d81c/torch_spline_conv-1.0.6.tar.gz\n",
      "Building wheels for collected packages: torch-spline-conv\n",
      "  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f6/e2/d7/d87645a1e1d34b51633b7fd13f020b618c0ee84000b4f4085c\n",
      "Successfully built torch-spline-conv\n",
      "Installing collected packages: torch-spline-conv\n",
      "Successfully installed torch-spline-conv-1.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch-spline-conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "colab_type": "code",
    "id": "uk6E9JP_Hvhj",
    "outputId": "5a986a43-4d09-41ab-efb6-9fb1cab56cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/4d/06dd0d277bf82ff4e1888f73b4c522c35f017505acacc25ecc95befaac6e/torch_geometric-1.1.0.tar.gz (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 2.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.14.6)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.1.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.20.3)\n",
      "Collecting plyfile (from torch-geometric)\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/82/d4069cbb49954d44087c37ff616cb423d3e2c0dd276378cdb4af3e3ef2ee/plyfile-0.7.tar.gz\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.0)\n",
      "Collecting rdflib (from torch-geometric)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
      "\u001b[K    100% |████████████████████████████████| 348kB 24.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.5.3)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.3.1)\n",
      "Collecting isodate (from rdflib->torch-geometric)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 14.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas->torch-geometric) (1.11.0)\n",
      "Building wheels for collected packages: torch-geometric, plyfile\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3e/c1/87/daa10b8fa568fb9a84ff1d9c24582af1ecc7e797e5e69ff58e\n",
      "  Building wheel for plyfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/91/3e/ee/e5630ef0fd53cedaa6e911ba27e8b40fff034388d1f264bb92\n",
      "Successfully built torch-geometric plyfile\n",
      "Installing collected packages: plyfile, isodate, rdflib, torch-geometric\n",
      "Successfully installed isodate-0.6.0 plyfile-0.7 rdflib-4.2.2 torch-geometric-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9q4iTifHvhl"
   },
   "source": [
    "### Install `proteinsolver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "colab_type": "code",
    "id": "HCWw1IgNHvhm",
    "outputId": "fcb1a352-1ca8-4708-c3af-11bb0f9a3c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://gitlab%2Bdeploy-token-52110:****@gitlab.com/ostrokach/proteinsolver.git\n",
      "  Cloning https://gitlab%2Bdeploy-token-52110:****@gitlab.com/ostrokach/proteinsolver.git to /tmp/pip-req-build-u2bdbexg\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: proteinsolver\n",
      "  Building wheel for proteinsolver (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-6rtg4lok/wheels/fb/05/26/209b40c24fc617333b311553f1c37610fb9b3475d67811bfcc\n",
      "Successfully built proteinsolver\n",
      "Installing collected packages: proteinsolver\n",
      "Successfully installed proteinsolver-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://gitlab.com/ostrokach/proteinsolver.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eoBMUoW2Hvhp",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "EmCKag-5MCiB",
    "outputId": "40059204-55bf-4285-eb0a-34098e980c40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  5 19:32:59 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN V             On   | 00000000:05:00.0  On |                  N/A |\n",
      "| 39%   59C    P2   104W / 250W |   4937MiB / 12033MiB |     77%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN Xp            On   | 00000000:06:00.0 Off |                  N/A |\n",
      "| 24%   43C    P8    10W / 250W |     12MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN Xp            On   | 00000000:09:00.0 Off |                  N/A |\n",
      "| 52%   81C    P2   221W / 250W |   8007MiB / 12196MiB |    100%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0       843      C   ...apkg-adjacency-net-v2-test81/bin/python  4807MiB |\n",
      "|    0      6913      G   /usr/lib/xorg/Xorg                            39MiB |\n",
      "|    0      6957      G   /usr/bin/gnome-shell                          78MiB |\n",
      "|    2      8438      C   ...apkg-adjacency-net-v2-test81/bin/python  7995MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbKxMUZWHvhq"
   },
   "outputs": [],
   "source": [
    "import atexit\n",
    "import csv\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from collections import deque\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import ChebConv, EdgeConv, GATConv, GCNConv\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, scatter_\n",
    "\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "rczy7pPiHvhs",
    "outputId": "8e4673d7-4cc7-4f8e-ffc4-de94302e5fab"
   },
   "outputs": [],
   "source": [
    "import proteinsolver\n",
    "import proteinsolver.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fFnAyUOHvhv"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_NAME = \"protein_4xEdgeConv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/kimlab1/strokach/working/proteinsolver/notebooks/protein_4xEdgeConv')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOTEBOOK_PATH = Path(NOTEBOOK_NAME).resolve()\n",
    "NOTEBOOK_PATH.mkdir(exist_ok=True)\n",
    "NOTEBOOK_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsEY3dtLHvhy",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/strokach/ml_data')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT = Path(tempfile.gettempdir())\n",
    "DATA_ROOT = Path(\"/home/strokach/ml_data\")\n",
    "DATA_ROOT.mkdir(exist_ok=True)\n",
    "DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_dataset_train_0 = proteinsolver.datasets.ProteinDataset(root=DATA_ROOT / \"protein_train_0\", subset=\"train_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_dataset_train_1 = proteinsolver.datasets.ProteinDataset(root=DATA_ROOT / \"protein_train_1\", subset=\"train_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protein_dataset_train_2 = proteinsolver.datasets.ProteinDataset(root=DATA_ROOT / \"protein_train_2\", subset=\"train_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_dataset_valid = proteinsolver.datasets.ProteinInMemoryDataset(root=DATA_ROOT / \"protein_valid\", subset=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_dataset_test = proteinsolver.datasets.ProteinInMemoryDataset(root=DATA_ROOT / \"protein_test\", subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/kimlab1/database_data/datapkg_output_dir/adjacency-net-v2/master/validation_dataset_wdistances/adjacency_matrix.parquet/database_id=G3DSA%3A2.40.155.10/part-00000-d5e89475-69dd-45c6-9a80-5bf751fce422-c000.snappy.parquet\"\n",
    "protein_dataset_gfp = proteinsolver.datasets.ProteinInMemoryDataset(root=DATA_ROOT / \"protein_gfp\", subset=\"gfp\", data_url=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SudokuDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZP3bLDO9PgT9"
   },
   "outputs": [],
   "source": [
    "sudoku_dataset_train = proteinsolver.datasets.SudokuDataset2(root=DATA_ROOT.joinpath(\"sudoku_train\"), subset=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudoku_dataset_valid = proteinsolver.datasets.SudokuDataset2(root=DATA_ROOT.joinpath(\"sudoku_valid\"), subset=\"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6W1sfkUHr89O"
   },
   "source": [
    "## `TUDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lOt0mVyir89P"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "Ctf7-3yYr89S",
    "outputId": "9ab47d4d-5cad-4f36-b32c-0f7b912d0e0a"
   },
   "outputs": [],
   "source": [
    "tu_dataset = TUDataset(root=tempfile.gettempdir() + '/ENZYMES', name='ENZYMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "tLtEuvRwr89Y",
    "outputId": "ca22011f-8035-4c70-c722-4ed39b1bdc17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 168], x=[37, 3], y=[1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3kWuRaxr89h"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGroupNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_groups, num_channels, eps=1e-05, affine=True):\n",
    "        super().__init__()\n",
    "        self.gn = nn.GroupNorm(num_groups=num_groups, num_channels=num_channels, eps=eps, affine=affine)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.t().unsqueeze(0)\n",
    "        x = self.gn(x)\n",
    "        return x.squeeze(0).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLocalResponseNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, size, alpha=0.0001, beta=0.75, k=1.0):\n",
    "        super().__init__()\n",
    "        self.lrn = nn.LocalResponseNorm(size=size, alpha=alpha, beta=beta, k=k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.t().unsqueeze(0)\n",
    "        x = self.lrn(x)\n",
    "        return x.squeeze(0).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72WWaQOXJDdr"
   },
   "outputs": [],
   "source": [
    "class EdgeConvMod(torch.nn.Module):\n",
    "    def __init__(self, nn, aggr=\"max\"):\n",
    "        super().__init__()\n",
    "        self.nn = nn\n",
    "        self.aggr = aggr\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        \"\"\"\"\"\"\n",
    "        row, col = edge_index\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "\n",
    "        # TODO: Try -x[col] instead of x[col] - x[row]\n",
    "        if edge_attr is None:\n",
    "            out = torch.cat([x[row], x[col]], dim=-1)\n",
    "        else:\n",
    "            out = torch.cat([x[row], x[col], edge_attr], dim=-1)\n",
    "        out = self.nn(out)\n",
    "        x = scatter_(self.aggr, out, row, dim_size=x.size(0))\n",
    "\n",
    "        return x, out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}(nn={})\".format(self.__class__.__name__, self.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDrlxheizsKg"
   },
   "outputs": [],
   "source": [
    "class EdgeConvBatch(nn.Module):\n",
    "    def __init__(self, gnn, hidden_size, batch_norm=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gnn = gnn\n",
    "\n",
    "        x_post_modules = []\n",
    "        edge_attr_post_modules = []\n",
    "\n",
    "        if batch_norm is not None:\n",
    "            x_post_modules.append(nn.BatchNorm1d(hidden_size, track_running_stats=False))\n",
    "            edge_attr_post_modules.append(nn.BatchNorm1d(hidden_size, track_running_stats=False))\n",
    "\n",
    "        if dropout:\n",
    "            x_post_modules.append(nn.Dropout(dropout))\n",
    "            edge_attr_post_modules.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.x_postprocess = nn.Sequential(*x_post_modules)\n",
    "        self.edge_attr_postprocess = nn.Sequential(*edge_attr_post_modules)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        x, edge_attr = self.gnn(x, edge_index, edge_attr)\n",
    "        x = self.x_postprocess(x)\n",
    "        edge_attr = self.edge_attr_postprocess(edge_attr)\n",
    "        return x, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNCaSlrFDGA6"
   },
   "outputs": [],
   "source": [
    "def get_graph_conv_layer(input_size, hidden_size, output_size):\n",
    "    mlp = nn.Sequential(\n",
    "        #\n",
    "        nn.Linear(input_size, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, output_size),\n",
    "    )\n",
    "    gnn = EdgeConvMod(nn=mlp, aggr=\"add\")\n",
    "    graph_conv = EdgeConvBatch(gnn, output_size, batch_norm=True, dropout=0.2)\n",
    "    return graph_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNCaSlrFDGA6"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, x_input_size, adj_input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_x = nn.Sequential(\n",
    "            nn.Embedding(x_input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size, track_running_stats=False),\n",
    "#             nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        if adj_input_size:\n",
    "            self.embed_adj = nn.Sequential(\n",
    "                nn.Linear(adj_input_size, hidden_size),\n",
    "                nn.ELU(),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size, track_running_stats=False),\n",
    "#                 nn.ELU(),\n",
    "            )\n",
    "        else:\n",
    "            self.embed_adj = None\n",
    "\n",
    "        self.graph_conv_1 = get_graph_conv_layer((2 + bool(adj_input_size)) * hidden_size, 2 * hidden_size, hidden_size)\n",
    "        self.graph_conv_2 = get_graph_conv_layer(3 * hidden_size, 2 * hidden_size, hidden_size)\n",
    "        self.graph_conv_3 = get_graph_conv_layer(3 * hidden_size, 2 * hidden_size, hidden_size)\n",
    "        self.graph_conv_4 = get_graph_conv_layer(3 * hidden_size, 2 * hidden_size, hidden_size)\n",
    "        self.linear_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "\n",
    "        x = self.embed_x(x)\n",
    "        edge_index, _ = remove_self_loops(edge_index)  # We should remove self loops in this case!\n",
    "        edge_attr = self.embed_adj(edge_attr) if edge_attr is not None else None\n",
    "\n",
    "        x_out, edge_attr_out = self.graph_conv_1(x, edge_index, edge_attr)\n",
    "        x += x_out\n",
    "        edge_attr = (edge_attr + edge_attr_out) if edge_attr is not None else edge_attr_out\n",
    "\n",
    "        x = F.relu(x)\n",
    "        edge_attr = F.relu(edge_attr)\n",
    "        x_out, edge_attr_out = self.graph_conv_2(x, edge_index, edge_attr)\n",
    "        x += x_out\n",
    "        edge_attr += edge_attr_out\n",
    "\n",
    "        x = F.relu(x)\n",
    "        edge_attr = F.relu(edge_attr)\n",
    "        x_out, edge_attr_out = self.graph_conv_3(x, edge_index, edge_attr)\n",
    "        x += x_out\n",
    "        edge_attr += edge_attr_out\n",
    "\n",
    "#         x = F.relu(x)\n",
    "#         edge_attr = F.relu(edge_attr)\n",
    "#         x_out, edge_attr_out = self.graph_conv_4(x, edge_index, edge_attr)\n",
    "#         x += x_out\n",
    "#         edge_attr += edge_attr_out\n",
    "\n",
    "        x = self.linear_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_fixed_width(lst, precision=None):\n",
    "    lst = [round(l, precision) if isinstance(l, float) else l for l in lst]\n",
    "    return [f\"{l: <18}\" for l in lst]\n",
    "\n",
    "\n",
    "class Stats:\n",
    "    epoch: int\n",
    "    step: int\n",
    "    batch_size: int\n",
    "    echo: bool\n",
    "    total_loss: float\n",
    "    num_correct_preds: int\n",
    "    num_preds: int\n",
    "    num_correct_preds_missing: int\n",
    "    num_preds_missing: int\n",
    "    num_correct_preds_missing_valid: int\n",
    "    num_preds_missing_valid: int\n",
    "    start_time: float\n",
    "\n",
    "    def __init__(self, *, epoch=0, step=0, batch_size=1, filename=None, echo=True):\n",
    "        self.epoch = epoch\n",
    "        self.step = step\n",
    "        self.batch_size = batch_size\n",
    "        self.echo = echo\n",
    "        self.reset_parameters()\n",
    "\n",
    "        if filename:\n",
    "            self.filehandle = open(filename, \"wt\", newline=\"\")\n",
    "            self.writer = csv.DictWriter(self.filehandle, list(self.stats.keys()), dialect=\"unix\")\n",
    "            self.writer.writeheader()\n",
    "            atexit.register(self.filehandle.close)\n",
    "        else:\n",
    "            self.filehandle = None\n",
    "            self.writer = None\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.num_steps = 0\n",
    "        self.total_loss = 0\n",
    "        self.num_correct_preds = 0\n",
    "        self.num_preds = 0\n",
    "        self.num_correct_preds_missing = 0\n",
    "        self.num_preds_missing = 0\n",
    "        self.num_correct_preds_missing_valid = 0\n",
    "        self.num_preds_missing_valid = 0\n",
    "        self.start_time = time.perf_counter()\n",
    "\n",
    "    @property\n",
    "    def header(self):\n",
    "        return \"\".join(to_fixed_width(self.stats.keys()))\n",
    "\n",
    "    @property\n",
    "    def row(self):\n",
    "        return \"\".join(to_fixed_width(self.stats.values(), 4))\n",
    "\n",
    "    @property\n",
    "    def stats(self):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return {\n",
    "                \"epoch\": self.epoch,\n",
    "                \"step\": self.step,\n",
    "                \"datapoint\": self.datapoint,\n",
    "                \"avg_loss\": np.float64(1) * self.total_loss / self.num_steps,\n",
    "                \"accuracy\": np.float64(1) * self.num_correct_preds / self.num_preds,\n",
    "                \"accuracy_m\": np.float64(1) * self.num_correct_preds_missing / self.num_preds_missing,\n",
    "                \"accuracy_mv\": np.float64(1) * self.num_correct_preds_missing_valid / self.num_preds_missing_valid,\n",
    "                \"time_elapsed\": time.perf_counter() - self.start_time,\n",
    "            }\n",
    "    \n",
    "    @property\n",
    "    def datapoint(self):\n",
    "        return self.step * self.batch_size\n",
    "\n",
    "    def write_header(self):\n",
    "        if self.echo:\n",
    "            print(self.header)\n",
    "        if self.writer is not None:\n",
    "            self.writer.writeheader()\n",
    "\n",
    "    def write_row(self):\n",
    "        if self.echo:\n",
    "            print(self.row)\n",
    "        if self.writer is not None:\n",
    "            self.writer.writerow(self.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = Stats(epoch=0, step=0, batch_size=64, filename=tempfile.NamedTemporaryFile().name, echo=True)\n",
    "# stats.num_steps += 1\n",
    "# stats.num_preds += 1\n",
    "# stats.num_preds_missing += 1\n",
    "# stats.num_preds_missing_valid += 1\n",
    "\n",
    "# stats.write_header()\n",
    "# stats.write_row()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "device = torch.device(\"cuda:1\")\n",
    "# device = \"cpu\"\n",
    "batch_size = 32\n",
    "num_features = 20\n",
    "adj_input_size = 2\n",
    "hidden_size = 128\n",
    "frac_present = 0.5\n",
    "frac_present_valid = frac_present\n",
    "info_size= 1024\n",
    "\n",
    "dataloaders = {\n",
    "    \"train_0\": DataLoader(protein_dataset_train_0, shuffle=True, num_workers=4, batch_size=batch_size, drop_last=True),\n",
    "    \"train_1\": DataLoader(protein_dataset_train_1, shuffle=True, num_workers=4, batch_size=batch_size, drop_last=True),\n",
    "    \"train_2\": None,\n",
    "    \"valid\": DataLoader(protein_dataset_valid[:128], shuffle=False, num_workers=4, batch_size=1, drop_last=False),\n",
    "    \"test\": DataLoader(protein_dataset_test[:128], shuffle=False, num_workers=4, batch_size=1, drop_last=False),\n",
    "\n",
    "    \"gfp_train\": DataLoader(protein_dataset_test[:928], shuffle=True, num_workers=4, batch_size=batch_size, drop_last=True),\n",
    "    \"gfp_valid\": DataLoader(protein_dataset_test[928:], shuffle=False, num_workers=4, batch_size=batch_size, drop_last=False),\n",
    "}\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6380
    },
    "colab_type": "code",
    "id": "azxPKTSKwcE8",
    "outputId": "09ed9b8b-25b9-49ff-8d02-83a4ec0ecba1"
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def eval_net(net: nn.Module):\n",
    "    training = net.training\n",
    "    try:\n",
    "        net.train(False)\n",
    "        yield\n",
    "    finally:\n",
    "        net.train(training)\n",
    "\n",
    "\n",
    "def get_stats_on_missing(data, output):\n",
    "    mask = (data.x == num_features).squeeze()\n",
    "    output_missing = output[mask]\n",
    "    _, predicted_missing = torch.max(output_missing.data, 1)\n",
    "    return (predicted_missing == data.y[mask]).sum().item(), len(predicted_missing)\n",
    "\n",
    "\n",
    "def get_data_x(data, frac_present):\n",
    "    x = torch.where(\n",
    "        torch.rand(data.y.size(0), device=device) < frac_present,\n",
    "        data.y,\n",
    "        torch.ones(1, dtype=torch.long, device=device) * num_features,\n",
    "    )\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6380
    },
    "colab_type": "code",
    "id": "azxPKTSKwcE8",
    "outputId": "09ed9b8b-25b9-49ff-8d02-83a4ec0ecba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch             step              datapoint         avg_loss          accuracy          accuracy_m        accuracy_mv       time_elapsed      \n",
      "0                 32                1024              2.4149            0.3424            0.0563            0.0738            16.6035           \n",
      "0                 64                2048              1.7405            0.5223            0.0908            0.0997            16.7236           \n",
      "0                 96                3072              1.6016            0.5344            0.1011            0.0974            16.4358           \n",
      "0                 128               4096              1.5445            0.5423            0.1051            0.1037            17.1874           \n",
      "0                 160               5120              1.5191            0.5468            0.1091            0.1051            17.3432           \n",
      "0                 192               6144              1.4969            0.5517            0.1115            0.1064            17.6504           \n",
      "0                 224               7168              1.4808            0.5521            0.1107            0.1095            16.9608           \n",
      "0                 256               8192              1.4843            0.551             0.1112            0.1078            17.3538           \n",
      "0                 288               9216              1.4773            0.5525            0.1159            0.1066            17.8606           \n",
      "0                 320               10240             1.4591            0.5578            0.1177            0.104             16.6614           \n",
      "0                 352               11264             1.4566            0.5558            0.1154            0.1052            17.4994           \n",
      "0                 384               12288             1.4418            0.5606            0.1182            0.1067            17.7397           \n",
      "0                 416               13312             1.4563            0.5542            0.115             0.11              17.5326           \n",
      "0                 448               14336             1.4446            0.557             0.1166            0.1069            16.9162           \n",
      "0                 480               15360             1.4476            0.5563            0.1153            0.1083            17.4301           \n",
      "0                 512               16384             1.4406            0.5578            0.1173            0.1119            17.2232           \n",
      "0                 544               17408             1.4359            0.558             0.1182            0.1103            16.772            \n",
      "0                 576               18432             1.4376            0.559             0.1213            0.1073            17.6235           \n",
      "0                 608               19456             1.4309            0.5592            0.1192            0.1083            17.5304           \n",
      "0                 640               20480             1.4242            0.5613            0.1213            0.1118            17.311            \n",
      "0                 672               21504             1.4291            0.5601            0.1217            0.1122            16.7003           \n",
      "0                 704               22528             1.4315            0.5588            0.1207            0.1116            17.5745           \n",
      "0                 736               23552             1.4349            0.557             0.118             0.1097            17.7325           \n",
      "0                 768               24576             1.4239            0.5607            0.1198            0.1122            17.7525           \n",
      "0                 800               25600             1.4291            0.5593            0.1214            0.1148            18.097            \n",
      "0                 832               26624             1.4231            0.5608            0.1203            0.1183            17.7283           \n",
      "0                 864               27648             1.4197            0.561             0.1218            0.1187            17.4634           \n",
      "0                 896               28672             1.4142            0.5628            0.1234            0.1134            17.6943           \n",
      "0                 928               29696             1.4229            0.5602            0.122             0.1174            16.4782           \n",
      "0                 960               30720             1.421             0.5587            0.1218            0.1186            17.367            \n",
      "0                 992               31744             1.4191            0.5602            0.1242            0.1151            17.1634           \n",
      "0                 1024              32768             1.4049            0.5638            0.1238            0.1189            17.3035           \n",
      "0                 1056              33792             1.4075            0.5632            0.1234            0.1143            17.2042           \n",
      "0                 1088              34816             1.4082            0.5638            0.1275            0.1185            17.6871           \n",
      "0                 1120              35840             1.4006            0.5658            0.1289            0.117             17.468            \n",
      "0                 1152              36864             1.4029            0.5649            0.1288            0.1195            17.7684           \n",
      "0                 1184              37888             1.4082            0.5626            0.1246            0.118             17.2829           \n",
      "0                 1216              38912             1.4056            0.5646            0.1264            0.119             17.8385           \n",
      "0                 1248              39936             1.4036            0.5647            0.1291            0.1242            16.7728           \n",
      "0                 1280              40960             1.4009            0.5659            0.1307            0.1237            16.8495           \n",
      "0                 1312              41984             1.3982            0.5669            0.1326            0.1175            17.4165           \n",
      "0                 1344              43008             1.403             0.5642            0.1313            0.1272            17.9983           \n",
      "0                 1376              44032             1.3976            0.5663            0.132             0.119             17.0417           \n",
      "0                 1408              45056             1.3961            0.5673            0.1339            0.1241            17.0315           \n",
      "0                 1440              46080             1.3971            0.5662            0.1343            0.1219            17.7219           \n",
      "0                 1472              47104             1.3976            0.5642            0.13              0.1296            18.0937           \n",
      "0                 1504              48128             1.3957            0.565             0.1297            0.1223            17.2373           \n",
      "0                 1536              49152             1.3963            0.5656            0.131             0.1237            16.5274           \n",
      "0                 1568              50176             1.3887            0.5686            0.1349            0.1293            17.2979           \n",
      "0                 1600              51200             1.3976            0.5654            0.1306            0.1251            17.8307           \n",
      "0                 1632              52224             1.3862            0.5679            0.1335            0.1301            17.7696           \n",
      "0                 1664              53248             1.3835            0.5682            0.1362            0.1307            17.4171           \n",
      "0                 1696              54272             1.3848            0.5684            0.1387            0.13              17.5079           \n",
      "0                 1728              55296             1.3894            0.5669            0.133             0.125             17.7785           \n",
      "0                 1760              56320             1.387             0.5673            0.1345            0.1336            17.6138           \n",
      "0                 1792              57344             1.3934            0.5668            0.1399            0.1318            17.0234           \n",
      "0                 1824              58368             1.3794            0.5693            0.1403            0.1342            17.2009           \n",
      "0                 1856              59392             1.3799            0.571             0.1403            0.1291            17.4599           \n",
      "0                 1888              60416             1.3762            0.5708            0.1387            0.1347            17.2838           \n",
      "0                 1920              61440             1.3759            0.5712            0.1401            0.1352            16.6658           \n",
      "0                 1952              62464             1.3809            0.5696            0.1389            0.1372            18.1536           \n",
      "0                 1984              63488             1.3738            0.5722            0.1424            0.129             17.7559           \n",
      "0                 2016              64512             1.3817            0.5695            0.1412            0.1275            17.7368           \n",
      "0                 2048              65536             1.3796            0.5691            0.1415            0.1352            17.0577           \n",
      "0                 2080              66560             1.3715            0.5716            0.1429            0.1287            17.2641           \n",
      "0                 2112              67584             1.3697            0.5732            0.1474            0.1328            17.756            \n",
      "0                 2144              68608             1.3751            0.5713            0.1411            0.1288            17.5874           \n",
      "0                 2176              69632             1.372             0.5717            0.1434            0.136             17.501            \n",
      "0                 2208              70656             1.3723            0.5713            0.1434            0.128             16.5214           \n",
      "0                 2240              71680             1.3688            0.5718            0.1412            0.1317            17.3665           \n",
      "0                 2272              72704             1.3614            0.5743            0.1474            0.1344            17.5598           \n",
      "0                 2304              73728             1.3631            0.5742            0.1467            0.1354            17.5823           \n",
      "0                 2336              74752             1.3694            0.5728            0.1474            0.1395            17.8345           \n",
      "0                 2368              75776             1.3558            0.5757            0.1481            0.1328            16.8929           \n",
      "0                 2400              76800             1.3818            0.5689            0.1422            0.1312            17.1502           \n",
      "0                 2432              77824             1.369             0.5725            0.1458            0.1335            17.0713           \n",
      "0                 2464              78848             1.3587            0.5752            0.1497            0.1381            17.2304           \n",
      "0                 2496              79872             1.3613            0.5747            0.1478            0.1398            17.746            \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-bf2814cdfb34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ci-datapkg-adjacency-net-v2-test81/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = Net(\n",
    "    x_input_size=num_features + 1, adj_input_size=adj_input_size, hidden_size=hidden_size, output_size=num_features\n",
    ")\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', verbose=True)\n",
    "\n",
    "net = net.train()\n",
    "stats = Stats(epoch=0, step=0, batch_size=batch_size, filename=NOTEBOOK_PATH.joinpath(\"training.log\"), echo=True)\n",
    "stats.write_header()\n",
    "for epoch in range(100_000):\n",
    "    stats.epoch = epoch\n",
    "    for i, data in enumerate(dataloaders[\"train_0\"]):\n",
    "        stats.step += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = data.to(device)\n",
    "        data.y = data.x\n",
    "        data.x = get_data_x(data, frac_present)\n",
    "        output = net(data.x, data.edge_index, data.edge_attr if hasattr(data, \"edge_attr\") else None)\n",
    "        \n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "\n",
    "        stats.total_loss += loss.detach().item()\n",
    "        stats.num_steps += 1\n",
    "\n",
    "        # Accuracy for all\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        stats.num_correct_preds += (predicted == data.y).sum().item()\n",
    "        stats.num_preds += len(predicted)\n",
    "\n",
    "        # Accuracy for missing only\n",
    "        num_correct, num_total = get_stats_on_missing(data, output)\n",
    "        stats.num_correct_preds_missing += num_correct\n",
    "        stats.num_preds_missing += num_total\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (stats.datapoint % info_size) < batch_size:\n",
    "            for j, data in enumerate(dataloaders[\"valid\"]):\n",
    "                data = data.to(device)\n",
    "                data.y = data.x\n",
    "                data.x = get_data_x(data, frac_present_valid)\n",
    "\n",
    "                with torch.no_grad() and eval_net(net):\n",
    "                    output = net(data.x, data.edge_index, data.edge_attr if hasattr(data, \"edge_attr\") else None)\n",
    "\n",
    "                num_correct, num_total = get_stats_on_missing(data, output)\n",
    "                stats.num_correct_preds_missing_valid += num_correct\n",
    "                stats.num_preds_missing_valid += num_total\n",
    "\n",
    "#             scheduler.step(stats.stats['accuracy'])\n",
    "            stats.write_row()\n",
    "            stats.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net(\n",
    "#     x_input_size=num_features + 1, adj_input_size=adj_input_size, hidden_size=hidden_size, output_size=num_features\n",
    "# )\n",
    "# net = net.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # criterion = nn.L1Loss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', verbose=True)\n",
    "\n",
    "# net = net.train()\n",
    "# stats = Stats(epoch=0, step=0, batch_size=batch_size, filename=NOTEBOOK_PATH.joinpath(\"training.log\"), echo=True)\n",
    "# stats.write_header()\n",
    "# for epoch in range(0, 10_000):\n",
    "#     stats.epoch = epoch\n",
    "    \n",
    "#     data_ref = next(iter(dataloaders[\"gfp_train\"]))\n",
    "#     data_ref = data_ref.to(device)\n",
    "    \n",
    "#     data_valid_ref = next(iter(dataloaders[\"gfp_test\"]))\n",
    "#     data_valid_ref = data_valid_ref.to(device)\n",
    "\n",
    "#     for i in range(100000):\n",
    "#         stats.step = i\n",
    "        \n",
    "#         data = data_ref.clone()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         if data.x is None:\n",
    "#             data.x = get_data_x(data, frac_present)\n",
    "#         output = net(data.x, data.edge_index, data.edge_attr if hasattr(data, \"edge_attr\") else None)\n",
    "        \n",
    "# #         y_onehot = torch.FloatTensor(data.y.size(0), num_features).to(device)\n",
    "# #         y_onehot.zero_()\n",
    "# #         y_onehot.scatter_(1, data.y.unsqueeze(1), 1.0)\n",
    "# #         loss = criterion(torch.sigmoid(output), y_onehot)\n",
    "\n",
    "#         loss = criterion(output, data.y)\n",
    "#         loss.backward()\n",
    "\n",
    "#         stats.total_loss += loss.detach().item()\n",
    "#         stats.num_steps += 1\n",
    "\n",
    "#         # Accuracy for all\n",
    "#         _, predicted = torch.max(output.data, 1)\n",
    "#         stats.num_correct_preds += (predicted == data.y).sum().item()\n",
    "#         stats.num_preds += len(predicted)\n",
    "\n",
    "#         # Accuracy for missing only\n",
    "#         num_correct, num_total = get_stats_on_missing(data, output)\n",
    "#         stats.num_correct_preds_missing += num_correct\n",
    "#         stats.num_preds_missing += num_total\n",
    "\n",
    "#         optimizer.step()\n",
    "    \n",
    "#         if (stats.num_processed_examples % info_size) < batch_size:\n",
    "#             data = data_valid_ref.clone()\n",
    "\n",
    "#             if data.x is None:\n",
    "#                 data.x = get_data_x(data, frac_present_valid)\n",
    "\n",
    "#             with torch.no_grad(): # and eval_net(net):\n",
    "#                 output = net(data.x, data.edge_index, data.edge_attr if hasattr(data, \"edge_attr\") else None)\n",
    "\n",
    "#             num_correct, num_total = get_stats_on_missing(data, output)\n",
    "#             stats.num_correct_preds_missing_valid += num_correct\n",
    "#             stats.num_preds_missing_valid += num_total\n",
    "\n",
    "#             scheduler.step(stats.stats['accuracy'])\n",
    "#             stats.write_row()\n",
    "#             stats.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1fXJz1l9HvhW",
    "u9q4iTifHvhl"
   ],
   "name": "Copy of 2019-03-30-sudoku-4xEdgeConv-09862+.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
